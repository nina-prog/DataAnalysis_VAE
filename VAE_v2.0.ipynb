{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_v1_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nina-prog/DataAnalysis_VAE/blob/main/VAE_v2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srk9OHyCZDzL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Reshape\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcUlg_PrMiyC"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9i4M00iwYs"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcrTqGPsZceo"
      },
      "source": [
        "### Load ecg5000 data using read_csv\n",
        "ecg5000 = pd.read_csv('ECG5000_ALL.txt', sep='\\s+', header=None)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbiFe4pZlQv"
      },
      "source": [
        "### Optional test and info about data set\n",
        "print(\"Type of ecg5000: \\t \\t {}\".format(type(ecg5000)))\n",
        "print(\"Dimensions of ecg5000: \\t \\t {}\".format(ecg5000.shape))\n",
        "print(\"Number of elements of ecg5000: \\t {}\".format((ecg5000.size)))\n",
        "print(\"Display first 10 rows of ecg5000: \\n {}\".format(ecg5000.head(10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvRjcSqjFED"
      },
      "source": [
        "### Normalize dataframe with min-max-normalization to range between [-0.8, 0.8] using sklearn MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-0.8,0.8))\n",
        "scaled_ecg5000 = pd.DataFrame(min_max_scaler.fit_transform(ecg5000))\n",
        "print(scaled_ecg5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Qxc-o8i2cV"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FuU0-HYavQx"
      },
      "source": [
        "### Split Data into 80/20 Training, Test\n",
        "trainDF, testDF = train_test_split(scaled_ecg5000, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Get all labels from trainDF and then drop it\n",
        "trainDF_Y = trainDF.iloc[:,0]\n",
        "trainDF.drop(trainDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# Get all labels from testDF and then drop it\n",
        "testDF_Y = testDF.iloc[:,0]\n",
        "testDF.drop(testDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# Optional test and info about new data sets\n",
        "print(\"Shape of Train DataFrame: \\t {}\".format(trainDF.shape))\n",
        "print(\"Shape of Test DataFrame: \\t {}\".format(testDF.shape))\n",
        "print(\"Shape of Train Y DataFrame: \\t {}\".format(trainDF_Y.shape))\n",
        "print(\"Shape of Test Y DataFrame: \\t {}\".format(testDF_Y.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqDAiFlPipdc"
      },
      "source": [
        "## Reshape Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkCjqkHiHBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cbdc18-eb8d-434f-ff9c-7bf8347d15b1"
      },
      "source": [
        "### Convert to array\n",
        "x_train = trainDF.to_numpy()\n",
        "x_test = testDF.to_numpy()\n",
        "\n",
        "y_train = trainDF_Y.to_numpy()\n",
        "y_test = testDF_Y.to_numpy()\n",
        "\n",
        "### Reshape datasets X/Y train/test into [samples, timesteps, features]\n",
        "s_xtrain = len(trainDF.index) # samples\n",
        "n_xtrain = len(trainDF.columns) # time steps\n",
        "\n",
        "s_xtest = len(testDF.index) # samples\n",
        "n_xtest = len(testDF.columns) # time steps\n",
        "\n",
        "s_ytrain = len(trainDF_Y.index) # samples\n",
        "\n",
        "s_ytest = len(testDF_Y.index) # samples\n",
        "\n",
        "x_train = x_train.reshape(s_xtrain, n_xtrain, 1)\n",
        "x_test = x_test.reshape(s_xtest, n_xtest, 1)\n",
        "\n",
        "y_train = y_train.reshape(s_ytrain, 1, 1)\n",
        "y_test = y_test.reshape(s_ytest, 1, 1)\n",
        "\n",
        "### Properties\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
        "\n",
        "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
        "print(\"Shape of y_test: {}\".format(y_test.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_train: (4000, 140, 1)\n",
            "Shape of x_test: (1000, 140, 1)\n",
            "Shape of y_train: (4000, 1, 1)\n",
            "Shape of y_test: (1000, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeXLn2bUYa7g"
      },
      "source": [
        "# Create Sample Layer\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuO7o5GGYg3y"
      },
      "source": [
        "class Sampling(layers.Layer):\r\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z\"\"\"\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var = inputs\r\n",
        "        batch = tf.shape(z_mean)[0]\r\n",
        "        dim = tf.shape(z_mean)[1]\r\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F9x-KyKM4NJ"
      },
      "source": [
        "# Build Variational Autoencoder (VAE)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmlHrviGtyZ"
      },
      "source": [
        "## Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_pSJx0oFlvB"
      },
      "source": [
        "### For better understanding visit: https://towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e\r\n",
        "### For better understanding of layers and Recreating auto encoders visit: https://machinelearningmastery.com/lstm-autoencoders/\r\n",
        "### or for code: https://gist.github.com/GerardBCN/40349b39bc45d4550141aff6966d1619#file-stock_price_autoencoding-ipynb\r\n",
        "### For Reshaping Issues: https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\r\n",
        "\r\n",
        "### Model Properties\r\n",
        "epochs = 12\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "input_shape=(None,140,1)\r\n",
        "\r\n",
        "encoding_dim = 140\r\n",
        "intermediate_dim = 140\r\n",
        "latent_dim = 5            #5, because 5 class in data ecg5000\r\n",
        "activation = 'tanh'\r\n",
        "dropout_rate = 0.2\r\n",
        "r_rate = 0.001\r\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hcpCe0zGwQl"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4194a4eCBSh5"
      },
      "source": [
        "def create_encoder(encoding_dim=140, intermediate_dim=140, latent_dim = 5, activation='tanh', dropout_rate=0.2, regulazier_rate=0.001): # maybe delete encoding and latent dim, do something with input shape\r\n",
        "    ### Define Layers\r\n",
        "    encoder_inputs = keras.Input(shape=(140, 1), name='Encoder_Input_layer')\r\n",
        "\r\n",
        "    encoded = Bidirectional(LSTM(intermediate_dim, activation=activation, name=''), name='Encode_1')(encoder_inputs)\r\n",
        "    encoded = Dropout(dropout_rate, name='Dropout_1')(encoded)\r\n",
        "    encoded = Dense(latent_dim, activation=activation, name='Encode_2', kernel_regularizer=l2(regulazier_rate), bias_regularizer=l2(regulazier_rate), activity_regularizer=l2(regulazier_rate))(encoded)\r\n",
        "\r\n",
        "    z_mean = Dense(latent_dim, activation=\"softplus\", name=\"z_mean\")(encoded) \r\n",
        "    z_log_var = Dense(latent_dim, activation=\"softplus\", name=\"z_log_var\")(encoded)\r\n",
        "\r\n",
        "    z = Sampling(name='Sample_layer')([z_mean, z_log_var])\r\n",
        "\r\n",
        "    ### Instantiate encoder\r\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\r\n",
        "\r\n",
        "    ### Configure encoder for training\r\n",
        "    encoder.compile(optimizer='adam', loss='mean_squared_error') # necessary? because we define train step in vae model\r\n",
        "\r\n",
        "    return encoder"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCpMI6C0Diyv"
      },
      "source": [
        "### Build Encoder # or only do create model later on\r\n",
        "encoder = create_encoder() \r\n",
        "encoder.summary()\r\n",
        "plot_model(encoder, show_shapes=True, to_file='reconstruct_lstm_encoder.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_PrqOCOG244"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxnUeH7FBJVH"
      },
      "source": [
        "def create_decoder(encoding_dim=140, intermediate_dim=140, latent_dim = 5, activation='tanh', dropout_rate=0.2, regulazier_rate=0.001): # maybe delete encoding and latent dim, do something with input shape\r\n",
        "    ### Define Layers\r\n",
        "    latent_inputs = keras.Input(shape=(latent_dim,), name='Decoder_Input_layer')\r\n",
        "\r\n",
        "    decoded = Dense(encoding_dim, activation=activation, name='Decode_1', kernel_regularizer=l2(regulazier_rate), bias_regularizer=l2(regulazier_rate), activity_regularizer=l2(regulazier_rate))(latent_inputs)\r\n",
        "    decoded = Reshape((140,1), name='Decode_2')(decoded)\r\n",
        "    decoded = Dropout(dropout_rate, name='Dropout_1')(decoded)\r\n",
        "    decoded = Bidirectional(LSTM(intermediate_dim, activation=activation, return_sequences=True, name=''), name='Decode_3')(decoded)\r\n",
        "\r\n",
        "    decoder_outputs = TimeDistributed(Dense(1, activation=activation, name=''),name='Decoder_Output_Layer')(decoded)\r\n",
        "\r\n",
        "    ### Instantiate decoder\r\n",
        "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\r\n",
        "\r\n",
        "    ### Configure encoder for training\r\n",
        "    decoder.compile(optimizer='adam', loss='mean_squared_error') # necessary? because we define train step in vae model\r\n",
        "\r\n",
        "    return decoder"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHOP8O9gFoR8"
      },
      "source": [
        "### Build Decoder # or only do create model later on\r\n",
        "decoder = create_decoder()\r\n",
        "decoder.summary()\r\n",
        "plot_model(decoder, show_shapes=True, to_file='reconstruct_lstm_decoder.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAKUqMR8oSm6"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNwUKMjdFM_L"
      },
      "source": [
        "Define VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9hxGlDYBBKV"
      },
      "source": [
        "class VAE(keras.Model):\r\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, encoder, decoder, **kwargs):\r\n",
        "        super(VAE, self).__init__(**kwargs)\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    @property\r\n",
        "    def metrics(self):\r\n",
        "        return [\r\n",
        "            self.total_loss_tracker,\r\n",
        "            self.reconstruction_loss_tracker,\r\n",
        "            self.kl_loss_tracker,\r\n",
        "        ]\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        if isinstance(data, tuple):\r\n",
        "            data = data[0]\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            z_mean, z_log_var, z = encoder(data)\r\n",
        "            reconstruction = decoder(z)\r\n",
        "            # reconstruction_loss = distance between Input and Output\r\n",
        "            reconstruction_loss = tf.reduce_mean(\r\n",
        "                #Alternative: keras.losses.binary_crossentropy(data, reconstruction)\r\n",
        "                keras.losses.mean_squared_error(data, reconstruction)\r\n",
        "            )\r\n",
        "            # kl_loss = distance between distributions and thus ensures the regular laten space\r\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\r\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\r\n",
        "            kl_loss *= -0.5\r\n",
        "            total_loss = reconstruction_loss + kl_loss\r\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\r\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\r\n",
        "        self.total_loss_tracker.update_state(total_loss) #new\r\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss) #new\r\n",
        "        self.kl_loss_tracker.update_state(kl_loss) #new\r\n",
        "        return {\r\n",
        "            \"loss\": self.total_loss_tracker.result(), #total_loss,\r\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(), #reconstruction_loss,\r\n",
        "            \"kl_loss\":  self.kl_loss_tracker.result(),#kl_loss,\r\n",
        "        }\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        # Unpack the data\r\n",
        "        x, y = data\r\n",
        "        # Compute predictions\r\n",
        "        y_pred = self(x, training=False)\r\n",
        "        # Updates the metrics tracking the loss\r\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\r\n",
        "        # Update the metrics.\r\n",
        "        self.compiled_metrics.update_state(y, y_pred)\r\n",
        "        # Return a dict mapping metric names to current value.\r\n",
        "        # Note that it will include the loss (tracked in self.metrics).\r\n",
        "        return {m.name: m.result() for m in self.metrics}\r\n",
        "\r\n",
        "    def call(self, data):\r\n",
        "        z_mean, z_log_var, z = self.encoder(data)\r\n",
        "        reconstructed = self.decoder(z)\r\n",
        "        return reconstructed"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWJVg5WPFQlb"
      },
      "source": [
        "Build VAE connecting Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfjVoHGu5eN_"
      },
      "source": [
        "### Function to create model, required for KerasClassifier\r\n",
        "def create_model(name, activation='tanh', intermediate_dim=140, dropout_rate=0.2, regulazier_rate=0.2, optimizer='Adam', learn_rate='0.001', momentum=0):\r\n",
        "    # create encoder \r\n",
        "    encoder = create_encoder(activation=activation, intermediate_dim=intermediate_dim, dropout_rate=dropout_rate, regulazier_rate=regulazier_rate)\r\n",
        "    # create decoder \r\n",
        "    decoder = create_decoder(activation=activation, intermediate_dim=intermediate_dim, dropout_rate=dropout_rate, regulazier_rate=regulazier_rate)\r\n",
        "    # create vae\r\n",
        "    vae = VAE(encoder, decoder, name=name)\r\n",
        "    # compile model\r\n",
        "    opt = optimizer(lr=learn_rate, momentum=momentum)\r\n",
        "    vae.compile(optimizer=opt)\r\n",
        "    return vae # possible to do encoder, decoder, vae?"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "g-XPM-YdEiOt",
        "outputId": "ea6b7344-2a79-4691-ce1e-f06ffd334adf"
      },
      "source": [
        "#vae = VAE(encoder, decoder, name=\"VAE\") # not necessary when using create_model()\r\n",
        "vae = create_model(name=\"VAE\")\r\n",
        "vae.build(input_shape)\r\n",
        "# vae.encoder.summary()\r\n",
        "# plot_model(encoder, show_shapes=True, to_file='reconstruct_lstm_encoder.png')\r\n",
        "# vae.decoder.summary()\r\n",
        "# plot_model(decoder, show_shapes=True, to_file='reconstruct_lstm_decoder.png')\r\n",
        "vae.summary(line_length=100)\r\n",
        "plot_model(vae, show_shapes=True, to_file='reconstruct_lstm_variational_autoencoder.png') # does not work correctly"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-508be43968b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#vae = VAE(encoder, decoder, name=\"VAE\") # not necessary when using create_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"VAE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# vae.encoder.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plot_model(encoder, show_shapes=True, to_file='reconstruct_lstm_encoder.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-5a99e2a8458d>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(name, activation, intermediate_dim, dropout_rate, regulazier_rate, optimizer, learn_rate, momentum)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintermediate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregulazier_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregulazier_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# create vae\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# compile model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'VAE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR-wmYsdQXII"
      },
      "source": [
        "### Instantiate VAE model\r\n",
        "# The output of vae model is the output of decoder in which its input is taken from the output of encoder !\r\n",
        "#decoder_outputs = decoder(encoder(encoder_inputs)[2])\r\n",
        "#vae = keras.Model(encoder_inputs, decoder_outputs, name='vae_v2_0')\r\n",
        "\r\n",
        "#vae.summary()\r\n",
        "#plot_model(vae, show_shapes=True, to_file='reconstruct_lstm_variational_autoencoder.png', rankdir='LR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7OlCIRCOUF-"
      },
      "source": [
        "# Train VAE\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfnzjwIxlShE"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTkdJdc_N5SN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013e16d1-3742-472c-c4a8-52af65f8eda7"
      },
      "source": [
        "### To improve training see here: https://becominghuman.ai/using-variational-autoencoder-vae-to-generate-new-images-14328877e88d\r\n",
        "### Train\r\n",
        "vae.compile(optimizer='Adam') #, loss='mean_squared_error' # not necessary when using create_model() earlier, because that functon already does compile\r\n",
        "history = vae.fit(x_train, x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, x_test))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "250/250 [==============================] - 105s 386ms/step - loss: 0.2041 - reconstruction_loss: 0.0308 - kl_loss: 0.1732 - val_loss: 0.0367\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 96s 383ms/step - loss: 0.0572 - reconstruction_loss: 0.0251 - kl_loss: 0.0321 - val_loss: 0.0358\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 96s 384ms/step - loss: 0.0374 - reconstruction_loss: 0.0239 - kl_loss: 0.0136 - val_loss: 0.0356\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 96s 382ms/step - loss: 0.0311 - reconstruction_loss: 0.0234 - kl_loss: 0.0077 - val_loss: 0.0358\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 96s 382ms/step - loss: 0.0281 - reconstruction_loss: 0.0231 - kl_loss: 0.0050 - val_loss: 0.0361\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 96s 385ms/step - loss: 0.0263 - reconstruction_loss: 0.0228 - kl_loss: 0.0035 - val_loss: 0.0364\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 96s 384ms/step - loss: 0.0252 - reconstruction_loss: 0.0226 - kl_loss: 0.0026 - val_loss: 0.0366\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 96s 384ms/step - loss: 0.0244 - reconstruction_loss: 0.0224 - kl_loss: 0.0020 - val_loss: 0.0367\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 97s 387ms/step - loss: 0.0238 - reconstruction_loss: 0.0223 - kl_loss: 0.0016 - val_loss: 0.0370\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 96s 385ms/step - loss: 0.0234 - reconstruction_loss: 0.0222 - kl_loss: 0.0012 - val_loss: 0.0371\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 96s 383ms/step - loss: 0.0231 - reconstruction_loss: 0.0221 - kl_loss: 0.0010 - val_loss: 0.0372\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 96s 385ms/step - loss: 0.0229 - reconstruction_loss: 0.0221 - kl_loss: 8.3169e-04 - val_loss: 0.0374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljaYW8HuuHZk"
      },
      "source": [
        "## Recreate Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYLbwD53cn84"
      },
      "source": [
        "# Encoder output is a list [z_mean, z_log_var, z] thus list[2] = z, see subsection encoder line 12\r\n",
        "\r\n",
        "### Extract myu i.e. z_mean\r\n",
        "z_mean = encoder.predict(x_test)[0]\r\n",
        "print(\"----- z_mean: -----\")\r\n",
        "print(z_mean)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract sigma i.e. z_log_var\r\n",
        "z_log_var = encoder.predict(x_test)[1]\r\n",
        "print(\"----- z_log_var: -----\")\r\n",
        "print(z_log_var)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract z_values and predict x_test\r\n",
        "z_values = encoder.predict(x_test)[2]\r\n",
        "# decoded_ecg5000 = vae.predict(x_test)\r\n",
        "decoded_ecg5000 = decoder.predict(z_values)\r\n",
        "\r\n",
        "# z_values contains list of each z_value per sample, i.e. we get 1000 SubLists with 5 elements in each.\r\n",
        "# Those 5 elements (z_values for Sample i) is our bottleneck which the decoder\r\n",
        "# recieves.\r\n",
        "print(\"----- z_values: -----\")\r\n",
        "print(z_values)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Properties\r\n",
        "print(\"Shape of z_mean: {}\".format(z_mean.shape))\r\n",
        "print(\"Shape of z_log_var: {}\".format(z_log_var.shape))\r\n",
        "print(\"Shape of decoded_ecg5000: {}\".format(decoded_ecg5000.shape))\r\n",
        "print(\"Shape of z_values: {}\".format(z_values.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNfzqeVE2N7J"
      },
      "source": [
        "## Display the training progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ufa2OwJIVe"
      },
      "source": [
        "### Loss vs Reconstruction_loss\r\n",
        "plt.figure(figsize=(16,8))\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['reconstruction_loss'])\r\n",
        "plt.legend([\"Loss\", \"Reconstruction Loss\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Reconstruction Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcIrsjRDYOiI"
      },
      "source": [
        "### Train loss vs val loss\r\n",
        "#Returns the loss value & metrics values for the model in test mode\r\n",
        "\r\n",
        "plt.figure(figsize=(16,8))\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Validation Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSBgUNCkM05C"
      },
      "source": [
        "#########################################################To-Do\r\n",
        "### Latent space\r\n",
        "# x,y Plot\r\n",
        "plt.figure(figsize=(15,6), linewidth=1)\r\n",
        "plt.plot(z_values[:,0], 'o')\r\n",
        "plt.plot(z_values[:,1], '+')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Scatterplot\r\n",
        "plt.figure(figsize=(15,12))\r\n",
        "plt.scatter(z_values[:,0], z_values[:,1], s=80, c=y_test, cmap='viridis') # or cmap=hsv\r\n",
        "plt.colorbar()\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ8dyTK6kNNQ"
      },
      "source": [
        "# Plot Data Results\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfVmINR3lji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19c4fe3-32ea-49b5-e11c-0557ee8337d3"
      },
      "source": [
        "### Test if Input fits Dim of Output\n",
        "print(\"Shape of Input x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of Output x_hat_train: {}\".format(decoded_ecg5000.shape))\n",
        "\n",
        "### Covert to 2D Array -- (\"-1\" = make a dimension (here rows) the size that will use the remaining unspecified elements)\n",
        "new_x_train= x_train.reshape(-1,140)\n",
        "new_decoded_ecg5000 = decoded_ecg5000.reshape(-1,140)\n",
        "\n",
        "print(\"Shape of Input after reshaping: {}\".format(new_x_train.shape))\n",
        "print(\"Shape of Output after reshaping: {}\".format(new_decoded_ecg5000.shape))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Input x_train: (4000, 140, 1)\n",
            "Shape of Output x_hat_train: (1000, 140, 1)\n",
            "Shape of Input after reshaping: (4000, 140)\n",
            "Shape of Output after reshaping: (1000, 140)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG129YUrHGyU"
      },
      "source": [
        "### Plot only one sample\r\n",
        "i = 934 # indize/sample which is going to be plotted\r\n",
        "plt.figure(linewidth = 1, figsize=(25,6))\r\n",
        "plt.title('Autoencoder Result')\r\n",
        "plt.xlabel('time steps')\r\n",
        "plt.plot(new_x_train[i], label='original ecg5000')\r\n",
        "plt.plot(new_decoded_ecg5000[i], label='decoded ecg5000')\r\n",
        "plt.legend(loc=\"upper left\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjUjaCtkSpW"
      },
      "source": [
        "### Plot Multiple Samples\n",
        "n_rows = 2                   \n",
        "n_cols = 2\n",
        "\n",
        "# Size Properties and layout design for tighter representation\n",
        "fig, axs = plt.subplots(nrows = n_rows, ncols = n_cols, figsize=(23,20))\n",
        "fig.tight_layout(w_pad=4, h_pad = 5)\n",
        "\n",
        "# Subplotting\n",
        "i = 100\n",
        "for row in range(n_rows):\n",
        "  for col in range(n_cols):\n",
        "    axs[row, col].plot(new_x_train[i])\n",
        "    axs[row, col].plot(new_decoded_ecg5000[i])\n",
        "    axs[row, col].legend([\"Original ECG5000 Sample {}\".format(i), \"Decoded ECG5000 Sample {}\".format(i)])\n",
        "    axs[row, col].set(xlabel = \"Time Steps\", ylabel = \"Heartbeat Interpolated\", title = \"Sample {}\".format(i))\n",
        "    i = i + 200\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unWrNuYBc71O"
      },
      "source": [
        "# Optimization\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX8k4aeUCx3k"
      },
      "source": [
        "## Hyperparameter (Sckit_GridSearchCV)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywVsad-OqE7a"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpKdEuTGRbgb"
      },
      "source": [
        "### To understand it better see here: https://mlfromscratch.com/gridsearch-keras-sklearn/#/\r\n",
        "### Define Function to gridSearch\r\n",
        "def gridSearch_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, model, param_grid, cv=10, scoring_fit='neg_mean_squared_error', do_probabilities = False):\r\n",
        "    # define gridSearch\r\n",
        "    gs = GridSearchCV(\r\n",
        "        estimator=model,\r\n",
        "        param_grid=param_grid, \r\n",
        "        cv=cv, \r\n",
        "        n_jobs=1, \r\n",
        "        scoring=scoring_fit,\r\n",
        "        verbose=2\r\n",
        "    )\r\n",
        "    # fit model\r\n",
        "    fitted_model = gs.fit(X_train_data, y_train_data)\r\n",
        "    # get all! rsults in a dataframe \r\n",
        "    df_result = pd.DataFrame(gs.cv_results_)\r\n",
        "    \r\n",
        "    if do_probabilities:\r\n",
        "      pred = fitted_model.predict_proba(X_test_data)\r\n",
        "    else:\r\n",
        "      pred = fitted_model.predict(X_test_data)\r\n",
        "    \r\n",
        "    return fitted_model, pred, df_result"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL_QfMgwCxfi"
      },
      "source": [
        "### Define evaluated params and it's value range\r\n",
        "param_grid = {\r\n",
        "              'epochs' :              [30, 50, 100],\r\n",
        "              'batch_size' :          [16, 32],\r\n",
        "              'activation' :          ['tanh', 'relu', 'sigmoid'],\r\n",
        "              'dropout_rate' :        [0.0, 0.2, 0.3],\r\n",
        "              'regulazier_rate' :     [0.001,0.004],\r\n",
        "              'optimizer' :           ['Adam', 'SGD'],\r\n",
        "              'learn_rate' :          [0.0, 0.001, 0.01, 0.1, 0.2, 0.3],\r\n",
        "              'momentum' :            [0.0, 0.2, 0.4, 0.6, 0.8, 0.9] \r\n",
        "             }\r\n",
        "#learning_rate?, kernel_regularizer?, bias_regularizer?, activity_regularizer?\r\n",
        "################ everthing except batch_size and epochs need to be naded as a parameter in create_model()\r\n",
        "################ grid param need to be parameters of create model? or have to be initiated in the model? dunno\r\n",
        "\r\n",
        "### Wrap keras custom VAE model with the KerasClassifier so it can be used in scikit-learn\r\n",
        "# see here how it is wrapped: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\r\n",
        "model = KerasClassifier(build_fn=create_model())\r\n",
        "\r\n",
        "### Run GridSearch\r\n",
        "# see here to solve error: https://stackoverflow.com/questions/23866833/whats-the-full-specification-for-implementing-a-custom-scikit-learn-estimator\r\n",
        "model, pred, df_GSresult = gridSearch_pipeline(x_train, x_test, y_train, y_test, model, param_grid, cv=5, scoring_fit='neg_log_loss')\r\n",
        "\r\n",
        "### Summarize results\r\n",
        "print(\"Best: %f using %s\" % (model.best_score_, model.best_params_))\r\n",
        "print(df_GSresult)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfBqVOpmdGJl"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZDGEMffcg0O"
      },
      "source": [
        "###Dropout_rate\r\n",
        "\r\n",
        "# configure the experiment\r\n",
        "def experiment_dropout():\r\n",
        "  # configure the experiment\r\n",
        "  n_dropout = [0.0, 0.2, 0.4, 0.6, 0.8]\r\n",
        "  # run the experiment\r\n",
        "  results = []\r\n",
        "  for drop_value in n_dropout:\r\n",
        "      # set dropout\r\n",
        "      drop_out_rate = drop_value\r\n",
        "      print(\"----- Dropout Rate: {} -----\".format(drop_out_rate))\r\n",
        "      # evaluate\r\n",
        "      # rather shorten code with defining a train function of code above and using it here\r\n",
        "      vae = VAE(encoder, decoder, name=\"VAE\")\r\n",
        "      vae.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "      history = vae.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), verbose=0)\r\n",
        "      # report performance\r\n",
        "      # rathr make a dataframe or something different which is simpler to plot\r\n",
        "      evaluation = []\r\n",
        "      evaluation.append(vae.evaluate(x_test, y_test))\r\n",
        "      evaluation.append(drop_value)\r\n",
        "\r\n",
        "      res = []\r\n",
        "      res.append(history.history[\"val_loss\"])\r\n",
        "      print(\"val_loss = {}\".format(res))\r\n",
        "      results.append(evaluation)\r\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfM-TqopGGUu"
      },
      "source": [
        "results = experiment_dropout()\r\n",
        "# summarize results\r\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}