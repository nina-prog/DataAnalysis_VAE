{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_v1_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nina-prog/DataAnalysis_VAE/blob/main/VAE_v2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srk9OHyCZDzL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Bidirectional, Dropout, Reshape, BatchNormalization, LeakyReLU, Flatten\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "\n",
        "sns.set()\n",
        "sns.set_palette(sns.color_palette(\"Set1\")) #tab10 #viridis\n",
        "#sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"paper\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRwBwVXkkKTt"
      },
      "source": [
        "tf.random.set_seed(7) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcUlg_PrMiyC"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9i4M00iwYs"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcrTqGPsZceo"
      },
      "source": [
        "### Load ecg5000 data using read_csv\n",
        "ecg5000 = pd.read_csv('ECG5000_ALL.txt', sep='\\s+', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbiFe4pZlQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac04a70-4ce7-4e81-aacc-9e6caa681afa"
      },
      "source": [
        "### Optional test and info about data set\n",
        "print(\"Type of ecg5000: \\t \\t {}\".format(type(ecg5000)))\n",
        "print(\"Dimensions of ecg5000: \\t \\t {}\".format(ecg5000.shape))\n",
        "print(\"Number of elements of ecg5000: \\t {}\".format((ecg5000.size)))\n",
        "print(\"Display first 10 rows of ecg5000: \\n {}\".format(ecg5000.head(10)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of ecg5000: \t \t <class 'pandas.core.frame.DataFrame'>\n",
            "Dimensions of ecg5000: \t \t (5000, 141)\n",
            "Number of elements of ecg5000: \t 705000\n",
            "Display first 10 rows of ecg5000: \n",
            "    0         1         2         3    ...       137       138       139       140\n",
            "0  1.0 -0.112522 -2.827204 -3.773897  ...  0.228077  0.123431  0.925286  0.193137\n",
            "1  1.0 -1.100878 -3.996840 -4.285843  ...  0.476333  0.773820  1.119621 -1.436250\n",
            "2  1.0 -0.567088 -2.593450 -3.874230  ... -0.532197  0.321097  0.904227 -0.421797\n",
            "3  1.0  0.490473 -1.914407 -3.616364  ...  0.990133  1.086798  1.403011 -0.383564\n",
            "4  1.0  0.800232 -0.874252 -2.384761  ...  0.960304  0.971020  1.614392  1.421456\n",
            "5  1.0 -1.507674 -3.574550 -4.478011  ...  1.007076  1.634990  1.493366 -0.783134\n",
            "6  1.0 -0.297161 -2.766635 -4.102185  ...  0.974787  1.110407  1.288165 -0.823386\n",
            "7  1.0  0.446769 -1.507397 -3.187468  ...  1.034388  1.258433  0.961215 -0.999476\n",
            "8  1.0  0.087631 -1.753490 -3.304473  ...  0.573453  0.192971 -0.648683 -2.441068\n",
            "9  1.0 -0.832281 -1.700368 -2.257301  ...  2.126372  2.126852  1.679299  0.965814\n",
            "\n",
            "[10 rows x 141 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvRjcSqjFED"
      },
      "source": [
        "# ### Normalize dataframe with min-max-normalization to range between [-0.8, 0.8] using sklearn MinMaxScaler\n",
        "# min_max_scaler = MinMaxScaler(feature_range=(-0.8,0.8))\n",
        "# scaled_ecg5000 = pd.DataFrame(min_max_scaler.fit_transform(ecg5000))\n",
        "# print(scaled_ecg5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Qxc-o8i2cV"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FuU0-HYavQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e65e9b-a249-4d2d-ebe9-672d7c312764"
      },
      "source": [
        "### Split Data into 80/20 Training, Test\n",
        "trainDF, testDF = train_test_split(ecg5000, test_size=0.2, shuffle=True, random_state=1)\n",
        "\n",
        "# get all labels from trainDF and then drop it\n",
        "trainDF_Y = trainDF.iloc[:,0]\n",
        "trainDF.drop(trainDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# get all labels from testDF and then drop it\n",
        "testDF_Y = testDF.iloc[:,0]\n",
        "testDF.drop(testDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# optional test and info about new data sets\n",
        "print(\"Shape of Train DataFrame: \\t {}\".format(trainDF.shape))\n",
        "print(\"Shape of Test DataFrame: \\t {}\".format(testDF.shape))\n",
        "print(\"Shape of Train Y DataFrame: \\t {}\".format(trainDF_Y.shape))\n",
        "print(\"Shape of Test Y DataFrame: \\t {}\".format(testDF_Y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Train DataFrame: \t (4000, 140)\n",
            "Shape of Test DataFrame: \t (1000, 140)\n",
            "Shape of Train Y DataFrame: \t (4000,)\n",
            "Shape of Test Y DataFrame: \t (1000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqDAiFlPipdc"
      },
      "source": [
        "## Reshape Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkCjqkHiHBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2da255-09ad-41fa-d290-f4e673406b75"
      },
      "source": [
        "### Convert to array\n",
        "x_train = trainDF.to_numpy()\n",
        "x_test = testDF.to_numpy()\n",
        "\n",
        "y_train = trainDF_Y.to_numpy()\n",
        "y_test = testDF_Y.to_numpy()\n",
        "\n",
        "### Reshape datasets X/Y train/test into [samples, timesteps, features]\n",
        "s_xtrain = len(trainDF.index) # samples\n",
        "n_xtrain = len(trainDF.columns) # time steps\n",
        "\n",
        "s_xtest = len(testDF.index) # samples\n",
        "n_xtest = len(testDF.columns) # time steps\n",
        "\n",
        "s_ytrain = len(trainDF_Y.index) # samples\n",
        "\n",
        "s_ytest = len(testDF_Y.index) # samples\n",
        "\n",
        "x_train = x_train.reshape(s_xtrain, n_xtrain, 1)\n",
        "x_test = x_test.reshape(s_xtest, n_xtest, 1)\n",
        "\n",
        "y_train = y_train.reshape(s_ytrain, 1, 1)\n",
        "y_test = y_test.reshape(s_ytest, 1, 1)\n",
        "\n",
        "### Properties\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
        "\n",
        "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
        "print(\"Shape of y_test: {}\".format(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_train: (4000, 140, 1)\n",
            "Shape of x_test: (1000, 140, 1)\n",
            "Shape of y_train: (4000, 1, 1)\n",
            "Shape of y_test: (1000, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeXLn2bUYa7g"
      },
      "source": [
        "# Sampling\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuO7o5GGYg3y"
      },
      "source": [
        "class Sampling(layers.Layer):\r\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z\"\"\"\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var = inputs\r\n",
        "        batch = tf.shape(z_mean)[0]\r\n",
        "        dim = tf.shape(z_mean)[1]\r\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F9x-KyKM4NJ"
      },
      "source": [
        "# Build Variational Autoencoder (VAE)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hcpCe0zGwQl"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4194a4eCBSh5"
      },
      "source": [
        "def create_encoder(encoding_dim=140, intermediate_dim=140, latent_dim=5, latent_activation='softplus', dropout_rate=0.2, regulazier_rate=0.004): # maybe delete encoding and latent dim, do something with input shape\r\n",
        "    \"\"\"Maps ECG5000 time series to a triplet (z_mean, z_log_var, z).\"\"\"\r\n",
        "\r\n",
        "    ### Define Layers\r\n",
        "    encoder_inputs = keras.Input(shape=(140, 1), name='Encoder_Input_layer')\r\n",
        "\r\n",
        "    encoded = Bidirectional(LSTM(intermediate_dim, activation='tanh', name=''), name='Encode_1')(encoder_inputs)\r\n",
        "    encoded = Dropout(dropout_rate, name='Dropout_1')(encoded)\r\n",
        "    encoded = Dense(latent_dim, activation='tanh', name='Encode_2', kernel_regularizer=l2(regulazier_rate), activity_regularizer=l2(regulazier_rate))(encoded)\r\n",
        "\r\n",
        "    z_mean = Dense(latent_dim, activation=latent_activation, name=\"z_mean\")(encoded)\r\n",
        "    z_log_var = Dense(latent_dim, activation=latent_activation, name=\"z_log_var\")(encoded)\r\n",
        "    z = Sampling(name='Sample_layer')([z_mean, z_log_var])\r\n",
        "\r\n",
        "    ### Instantiate encoder\r\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\r\n",
        "\r\n",
        "    return encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCpMI6C0Diyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2ea663-5ed9-4c06-97d3-b3cd42c891a8"
      },
      "source": [
        "### Check if encoder works\r\n",
        "encoder_test = create_encoder() \r\n",
        "encoder_test.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Encoder_Input_layer (InputLayer [(None, 140, 1)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encode_1 (Bidirectional)        (None, 280)          159040      Encoder_Input_layer[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "Dropout_1 (Dropout)             (None, 280)          0           Encode_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Encode_2 (Dense)                (None, 5)            1405        Dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 5)            30          Encode_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 5)            30          Encode_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Sample_layer (Sampling)         (None, 5)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 160,505\n",
            "Trainable params: 160,505\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_PrqOCOG244"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxnUeH7FBJVH"
      },
      "source": [
        "def create_decoder(encoding_dim=140, intermediate_dim=140, latent_dim=5, dropout_rate=0.2, regulazier_rate=0.004): # maybe delete encoding and latent dim, do something with input shape\r\n",
        "    \"\"\"Converts z, the encoded time series, back into a readable time series.\"\"\"\r\n",
        "    \r\n",
        "    ### Define Layers\r\n",
        "    latent_inputs = keras.Input(shape=(latent_dim,), name='Decoder_Input_layer')\r\n",
        "\r\n",
        "    decoded = Dense(encoding_dim*256, activation='tanh', name='Decode_1', kernel_regularizer=l2(regulazier_rate), activity_regularizer=l2(regulazier_rate))(latent_inputs)\r\n",
        "    decoded = Reshape((140,256), name='Decode_2')(decoded)\r\n",
        "    decoded = Dropout(dropout_rate, name='Dropout_1')(decoded)\r\n",
        "    decoded = Bidirectional(LSTM(intermediate_dim, activation='tanh', return_sequences=True, name=''), name='Decode_3')(decoded)\r\n",
        "\r\n",
        "    decoder_outputs = TimeDistributed(Dense(1, activation='linear', name=''),name='Decoder_Output_Layer')(decoded)\r\n",
        "    \r\n",
        "    ### Instantiate decoder\r\n",
        "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\r\n",
        "\r\n",
        "    return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHOP8O9gFoR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a8e4b5-8f20-472d-80a1-704bf46c24dd"
      },
      "source": [
        "### Check if decoder works\r\n",
        "decoder_test = create_decoder()\r\n",
        "decoder_test.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Decoder_Input_layer (InputLa [(None, 5)]               0         \n",
            "_________________________________________________________________\n",
            "Decode_1 (Dense)             (None, 35840)             215040    \n",
            "_________________________________________________________________\n",
            "Decode_2 (Reshape)           (None, 140, 256)          0         \n",
            "_________________________________________________________________\n",
            "Dropout_1 (Dropout)          (None, 140, 256)          0         \n",
            "_________________________________________________________________\n",
            "Decode_3 (Bidirectional)     (None, 140, 280)          444640    \n",
            "_________________________________________________________________\n",
            "Decoder_Output_Layer (TimeDi (None, 140, 1)            281       \n",
            "=================================================================\n",
            "Total params: 659,961\n",
            "Trainable params: 659,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAKUqMR8oSm6"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNwUKMjdFM_L"
      },
      "source": [
        "### Define VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9hxGlDYBBKV"
      },
      "source": [
        "class VAE(keras.Model):\r\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, encoder, decoder, **kwargs):\r\n",
        "        super(VAE, self).__init__(**kwargs)\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        # unpack the data\r\n",
        "        if isinstance(data, tuple):\r\n",
        "            data = data[0]\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            # forward pass\r\n",
        "            z_mean, z_log_var, z = self.encoder(data)\r\n",
        "            reconstruction = self.decoder(z)\r\n",
        "            # Compute own loss\r\n",
        "            reconstruction_loss = tf.reduce_mean(\r\n",
        "                keras.losses.mean_squared_error(data, reconstruction)*140\r\n",
        "            )\r\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\r\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\r\n",
        "            kl_loss *= -0.5\r\n",
        "            total_loss = reconstruction_loss + kl_loss\r\n",
        "        # compute gradients\r\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\r\n",
        "        # update weights\r\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\r\n",
        "        # compute own metrics\r\n",
        "        return {\r\n",
        "            \"loss\": total_loss,\r\n",
        "            \"reconstruction_loss\": reconstruction_loss,\r\n",
        "            \"kl_loss\": kl_loss,\r\n",
        "        }\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        # unpack the data\r\n",
        "        x, y = data\r\n",
        "        # compute predictions\r\n",
        "        y_pred = self(x, training=False)\r\n",
        "        # updates the metrics tracking the loss\r\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\r\n",
        "        # update the metrics\r\n",
        "        self.compiled_metrics.update_state(y, y_pred)\r\n",
        "        # return a dict mapping metric names to current value\r\n",
        "        return {m.name: m.result() for m in self.metrics}\r\n",
        "\r\n",
        "    def call(self, data):\r\n",
        "        z_mean, z_log_var, z = self.encoder(data)\r\n",
        "        reconstructed = self.decoder(z)\r\n",
        "        return reconstructed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWJVg5WPFQlb"
      },
      "source": [
        "### Build VAE connecting Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfjVoHGu5eN_"
      },
      "source": [
        "### Definde function to create model\r\n",
        "def create_model(latent_activation='softplus', intermediate_dim=140, dropout_rate=0.2, regulazier_rate=0.004, optimizer='adam', learn_rate=0.001, name='VAE'):\r\n",
        "    \"\"\"Creates VAE model, required for wrapping in estimator interface KerasRegressor, while accepting the hyperparameters we want to tune. We also pass some default values.\"\"\"\r\n",
        "    \r\n",
        "    # create encoder \r\n",
        "    encoder = create_encoder(latent_activation=latent_activation, intermediate_dim=intermediate_dim, dropout_rate=dropout_rate, regulazier_rate=regulazier_rate)\r\n",
        "    # create decoder \r\n",
        "    decoder = create_decoder(intermediate_dim=intermediate_dim, dropout_rate=dropout_rate, regulazier_rate=regulazier_rate)\r\n",
        "    # create vae\r\n",
        "    model = VAE(encoder, decoder, name=name)\r\n",
        "    # compile model\r\n",
        "    if optimizer == 'adam':\r\n",
        "      opt = Adam(lr=learn_rate, amsgrad=True)\r\n",
        "    else:\r\n",
        "      opt = SGD(lr=learn_rate)\r\n",
        "    model.compile(optimizer=opt)\r\n",
        "    model.build((None,140,1))\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-XPM-YdEiOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb8ec8d-d0bf-46ab-9a6a-494f35bded9d"
      },
      "source": [
        "### Instantiate VAE model\r\n",
        "vae = create_model(name='VAE')\r\n",
        "\r\n",
        "### Display VAE model and it`s parts\r\n",
        "# encoder \r\n",
        "vae.encoder.summary(line_length=100)\r\n",
        "plot_model(vae.encoder, show_shapes=True, to_file='vae_encoder.png')\r\n",
        "print(\"\\n\")\r\n",
        "# decoder\r\n",
        "vae.decoder.summary(line_length=100)\r\n",
        "plot_model(vae.decoder, show_shapes=True, to_file='vae_decoder.png')\r\n",
        "print(\"\\n\")\r\n",
        "# vae\r\n",
        "vae.summary(line_length=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                     Output Shape          Param #     Connected to                     \n",
            "====================================================================================================\n",
            "Encoder_Input_layer (InputLayer) [(None, 140, 1)]      0                                            \n",
            "____________________________________________________________________________________________________\n",
            "Encode_1 (Bidirectional)         (None, 280)           159040      Encoder_Input_layer[0][0]        \n",
            "____________________________________________________________________________________________________\n",
            "Dropout_1 (Dropout)              (None, 280)           0           Encode_1[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "Encode_2 (Dense)                 (None, 5)             1405        Dropout_1[0][0]                  \n",
            "____________________________________________________________________________________________________\n",
            "z_mean (Dense)                   (None, 5)             30          Encode_2[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "z_log_var (Dense)                (None, 5)             30          Encode_2[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "Sample_layer (Sampling)          (None, 5)             0           z_mean[0][0]                     \n",
            "                                                                   z_log_var[0][0]                  \n",
            "====================================================================================================\n",
            "Total params: 160,505\n",
            "Trainable params: 160,505\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Model: \"decoder\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                                 Output Shape                            Param #        \n",
            "====================================================================================================\n",
            "Decoder_Input_layer (InputLayer)             [(None, 5)]                             0              \n",
            "____________________________________________________________________________________________________\n",
            "Decode_1 (Dense)                             (None, 35840)                           215040         \n",
            "____________________________________________________________________________________________________\n",
            "Decode_2 (Reshape)                           (None, 140, 256)                        0              \n",
            "____________________________________________________________________________________________________\n",
            "Dropout_1 (Dropout)                          (None, 140, 256)                        0              \n",
            "____________________________________________________________________________________________________\n",
            "Decode_3 (Bidirectional)                     (None, 140, 280)                        444640         \n",
            "____________________________________________________________________________________________________\n",
            "Decoder_Output_Layer (TimeDistributed)       (None, 140, 1)                          281            \n",
            "====================================================================================================\n",
            "Total params: 659,961\n",
            "Trainable params: 659,961\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Model: \"VAE\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                                 Output Shape                            Param #        \n",
            "====================================================================================================\n",
            "encoder (Functional)                         [(None, 5), (None, 5), (None, 5)]       160505         \n",
            "____________________________________________________________________________________________________\n",
            "decoder (Functional)                         (None, 140, 1)                          659961         \n",
            "====================================================================================================\n",
            "Total params: 820,466\n",
            "Trainable params: 820,466\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7OlCIRCOUF-"
      },
      "source": [
        "# Train VAE\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf2gXbw290r5"
      },
      "source": [
        "### Train Properties\r\n",
        "epochs = 100 #50, 100\r\n",
        "batch_size = 16 #16, 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfnzjwIxlShE"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTkdJdc_N5SN"
      },
      "source": [
        "### Train\r\n",
        "train_history = vae.fit(x_train, x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EOIKwwPnPgT"
      },
      "source": [
        "### Save history\r\n",
        "with open('/trainHistoryDict', 'wb') as file_pi:\r\n",
        "        pickle.dump(train_history.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHZuTNLVjUeh"
      },
      "source": [
        "### Check displayed values in the command line with actual output values of the trainings process\r\n",
        "print(\"----- loss: -----\\n{}\".format(train_history.history[\"loss\"]))\r\n",
        "print(\"----- reconstruction_loss: -----\\n{}\".format(train_history.history[\"reconstruction_loss\"]))\r\n",
        "print(\"----- kl_loss: -----\\n{}\".format(train_history.history[\"kl_loss\"]))\r\n",
        "print(\"----- val_loss: -----\\n{}\".format(train_history.history[\"val_loss\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljaYW8HuuHZk"
      },
      "source": [
        "## Recreate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYLbwD53cn84"
      },
      "source": [
        "# Encoder output is a list [z_mean, z_log_var, z] thus list[2] = z, see subsection encoder line 12\r\n",
        "\r\n",
        "### Extract myu i.e. z_mean\r\n",
        "z_mean = vae.encoder.predict(x_test)[0]\r\n",
        "print(\"----- z_mean: -----\")\r\n",
        "print(z_mean)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract sigma i.e. z_log_var\r\n",
        "z_log_var = vae.encoder.predict(x_test)[1]\r\n",
        "print(\"----- z_log_var: -----\")\r\n",
        "print(z_log_var)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract z_values and predict x_test\r\n",
        "z_values = vae.encoder.predict(x_test)[2]\r\n",
        "decoded_ecg5000 = vae.decoder.predict(z_values)\r\n",
        "# z_values contains list of each z_value per sample, i.e. we get 1000 SubLists with 5 elements in each.\r\n",
        "# Those 5 elements (z_values for Sample i) is our bottleneck which the decoder recieves.\r\n",
        "print(\"----- z_values: -----\")\r\n",
        "print(z_values)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Save extracted values\r\n",
        "np.savetxt('z_values.csv', z_values, delimiter=\",\")\r\n",
        "np.savetxt('decoded_ecg5000.csv', decoded_ecg5000.reshape(-1,140), delimiter=\",\")\r\n",
        "\r\n",
        "### Properties\r\n",
        "print(\"Shape and Type of z_mean: {}, {}\".format(z_mean.shape, type(z_mean)))\r\n",
        "print(\"Shape and Type of z_log_var: {}, {}\".format(z_log_var.shape, type(z_log_var)))\r\n",
        "print(\"Shape and Type of z_values: {}, {}\".format(z_values.shape, type(z_values)))\r\n",
        "print(\"Shape and Type of decoded_ecg5000: {}, {}\".format(decoded_ecg5000.shape, type(decoded_ecg5000)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNfzqeVE2N7J"
      },
      "source": [
        "## Display the training progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPE8qAylUlhq"
      },
      "source": [
        "#### Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ufa2OwJIVe"
      },
      "source": [
        "### Loss vs Reconstruction_loss vs KL Divergence\r\n",
        "plt.figure(figsize=(8,5))\r\n",
        "plt.plot(train_history.history['loss'])\r\n",
        "plt.plot(train_history.history['reconstruction_loss'])\r\n",
        "plt.plot(train_history.history['kl_loss'])\r\n",
        "plt.legend([\"Loss\", \"Reconstruction Loss\", \"KL Divergence\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Reconstruction Loss vs. KL Divergence\")\r\n",
        "\r\n",
        "plt.savefig('loss.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcIrsjRDYOiI"
      },
      "source": [
        "### Train loss vs val loss\r\n",
        "# returns the loss value & metrics values for the model in test mode\r\n",
        "plt.figure(figsize=(8,5))\r\n",
        "plt.plot(train_history.history['loss'])\r\n",
        "plt.plot(train_history.history['val_loss'])\r\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Validation Loss\")\r\n",
        "\r\n",
        "plt.savefig('valLoss.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGsZngmEUnKf"
      },
      "source": [
        "### Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldOtLNeJYJmb"
      },
      "source": [
        "### Scale Datan (PCA)\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "# transform to dataframe\r\n",
        "z_test = pd.DataFrame(z_values)\r\n",
        "# standardize the data\r\n",
        "z_test = StandardScaler().fit_transform(z_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksY0r5zQsVjn"
      },
      "source": [
        "### Estimate how many components are needed to describe the data (PCA)\r\n",
        "pca_explained = PCA().fit(z_test)\r\n",
        "plt.plot(np.cumsum(pca_explaoned.explained_variance_ratio_))\r\n",
        "plt.xlabel('number of components')\r\n",
        "plt.ylabel('cumulative explained variance');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjILQXppYgIP"
      },
      "source": [
        "### PCA (5 dim -> 2 dim): display a 2D plot of the classes in the latent space.\r\n",
        "# make PCA instance\r\n",
        "pca = PCA(n_components=2)\r\n",
        "# fit transform features\r\n",
        "principalComponents = pca.fit_transform(z_test)\r\n",
        "# build pca dataframe\r\n",
        "principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\r\n",
        "targetDF = pd.DataFrame(data=testDF_Y.to_numpy(), columns=['target'])\r\n",
        "finalDF = pd.concat([principalDf, targetDF], axis = 1)\r\n",
        "# scatterplot\r\n",
        "plt.figure(figsize=(8,5))\r\n",
        "plt.xlabel('Principal Component 1')\r\n",
        "plt.ylabel('Principal Component 2')\r\n",
        "plt.title('Principal Component Analysis of Latent Space')\r\n",
        "plt.scatter(finalDF['principal component 1'], finalDF['principal component 2'], c=finalDF['target'], cmap=plt.cm.get_cmap('Set1', 6), s=40, alpha=0.7) # or cmap=hsv\r\n",
        "plt.colorbar(ticks=range(6), label='Classes of ECG500')\r\n",
        "plt.clim(-0.5, 5.5)\r\n",
        "\r\n",
        "plt.show()\r\n",
        "plt.savefig('PCA.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ8dyTK6kNNQ"
      },
      "source": [
        "# Plot Data Results\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfVmINR3lji"
      },
      "source": [
        "### Test if Input fits Dim of Output\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of decoded_ecg5000: {}\".format(decoded_ecg5000.shape))\n",
        "\n",
        "### Covert to 2D Array (\"-1\" = make a dimension (here rows) the size that will use the remaining unspecified elements)\n",
        "new_x_train= x_train.reshape(-1,140)\n",
        "new_decoded_ecg5000 = decoded_ecg5000.reshape(-1,140)\n",
        "\n",
        "print(\"Shape of Input after reshaping: {}\".format(new_x_train.shape))\n",
        "print(\"Shape of Output after reshaping: {}\".format(new_decoded_ecg5000.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG129YUrHGyU"
      },
      "source": [
        "# ### Plot figure for paper\r\n",
        "# i = 934 # indize/sample which is going to be plotted\r\n",
        "# plt.figure(linewidth = 1, figsize=(25,6))\r\n",
        "# plt.xlabel('time steps')\r\n",
        "# plt.plot(new_x_train[i])\r\n",
        "# plt.show()\r\n",
        "# plt.savefig('diagramm_original.jpg')\r\n",
        "\r\n",
        "# plt.figure(linewidth = 1, figsize=(25,6))\r\n",
        "# plt.xlabel('time steps')\r\n",
        "# plt.plot(new_decoded_ecg5000[i], label='decoded ecg5000')\r\n",
        "# plt.show()\r\n",
        "# plt.savefig('diagramm_decoded.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGvzjGwcJi1B"
      },
      "source": [
        "### Plot only one sample\r\n",
        "i = 901 # indize/sample which is going to be plotted\r\n",
        "plt.figure(linewidth = 1, figsize=(20,6))\r\n",
        "plt.title('Autoencoder Result')\r\n",
        "plt.xlabel('time steps')\r\n",
        "plt.plot(new_decoded_ecg5000[i], label='decoded ecg5000')\r\n",
        "plt.plot(new_x_train[i], label='original ecg5000')\r\n",
        "plt.legend(loc=\"upper left\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjUjaCtkSpW"
      },
      "source": [
        "### Plot Multiple Samples\n",
        "n_rows = 2                   \n",
        "n_cols = 3\n",
        "\n",
        "# size properties and layout design for tighter representation\n",
        "fig, axs = plt.subplots(nrows = n_rows, ncols = n_cols, figsize=(13,6))\n",
        "fig.tight_layout(w_pad=4, h_pad = 5)\n",
        "\n",
        "# subplotting\n",
        "i = 50\n",
        "for row in range(n_rows):\n",
        "  for col in range(n_cols):\n",
        "    axs[row, col].plot(new_decoded_ecg5000[i])\n",
        "    axs[row, col].plot(new_x_train[i])\n",
        "    axs[row, col].legend([\"Decoded ECG5000 Sample {}\".format(i), \"Original ECG5000 Sample {}\".format(i)])\n",
        "    axs[row, col].set(xlabel = \"Time Steps\", ylabel = \"Heartbeat Interpolated\", title = \"Sample {}\".format(i))\n",
        "    i = i + 75\n",
        "\n",
        "plt.savefig('dataComparison.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unWrNuYBc71O"
      },
      "source": [
        "# Optimization\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX8k4aeUCx3k"
      },
      "source": [
        "## Hyperparameter (Sckit_GridSearchCV)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywVsad-OqE7a"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "from sklearn.metrics import mean_squared_error, make_scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3NsIDD4dWHN"
      },
      "source": [
        "### Define scorer\r\n",
        "def score_mse(y_true, y_pred): # , **kwargs\r\n",
        "    \"\"\"Implementing mean squarred error as a score for RandomizedSearchCV.\"\"\"\r\n",
        "\r\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\r\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\r\n",
        "    # removing all size 1 dimensions in y_true\r\n",
        "    y_true = tf.squeeze(y_true)\r\n",
        "    return np.mean(tf.math.squared_difference(y_pred, y_true))\r\n",
        "\r\n",
        "### Define Function for Randomized Search \r\n",
        "def randomizedSearch_pipeline(x_train_data, x_test_data, model, space, n_iter=10, scoring_fit='neg_mean_squared_error', cv=5, do_probabilities = False):\r\n",
        "    \"\"\"Pipeline for RandomizedSearchCV: Select settings and run randomizedSearchCV, returning results.\"\"\"\r\n",
        "    # define randomizedSearch\r\n",
        "    rs = RandomizedSearchCV(\r\n",
        "        estimator=model,\r\n",
        "        param_distributions=space, \r\n",
        "        n_iter=n_iter,\r\n",
        "        scoring=scoring_fit,\r\n",
        "        n_jobs=1, \r\n",
        "        cv=cv, \r\n",
        "        verbose=2, \r\n",
        "        random_state=1,\r\n",
        "    )\r\n",
        "    # fit model\r\n",
        "    fitted_model = rs.fit(x_train_data, x_train_data, verbose=0)\r\n",
        "    # get results\r\n",
        "    rs_result = pd.DataFrame(rs.cv_results_)\r\n",
        "    # save compromised version of the results\r\n",
        "    min_rs_results = pd.concat([pd.DataFrame(rs.cv_results_[\"mean_test_score\"], columns=[\"score\"]),\r\n",
        "                                pd.DataFrame(rs.cv_results_[\"params\"])], axis=1)\r\n",
        "    min_rs_results = min_rs_results.sort_values(by=\"score\", ascending=False)\r\n",
        "    min_rs_results.to_latex(buf='randomizedSearchResults.tex', caption=(\"Results of 20 candiates using a cross-validation of 5\", \"Randomized Search Results\"), label='table:1')\r\n",
        "    \r\n",
        "    if do_probabilities:\r\n",
        "      pred = fitted_model.predict_proba(x_test_data)\r\n",
        "    else:\r\n",
        "      pred = fitted_model.predict(x_test_data)\r\n",
        "    \r\n",
        "    return fitted_model, pred, rs_result, min_rs_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL_QfMgwCxfi"
      },
      "source": [
        "### Define evaluated params and it's value range\r\n",
        "space = {\r\n",
        "        #'epochs' :              [50, 100],\r\n",
        "        #'latent_activation' :   ['softplus','softmax','sigmoid'],\r\n",
        "        #'optimizer' :           ['adam', 'SGD'],\r\n",
        "        'batch_size' :          list(np.logspace(0,6,7, base=2, dtype=int)),\r\n",
        "        'dropout_rate' :        list(np.linspace(0, 1)),\r\n",
        "        'regulazier_rate' :     list(np.logspace(-6,-1,6)),\r\n",
        "        'learn_rate' :          list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 100)),\r\n",
        "        }\r\n",
        "\r\n",
        "### Wrap keras custom VAE model with the KerasClassifier thus it implements estimator interface\r\n",
        "model = KerasRegressor(build_fn=create_model)\r\n",
        "\r\n",
        "### Run RandomizedSearch\r\n",
        "fitted_model, pred, rs_result, min_rs_results= randomizedSearch_pipeline(x_train, x_test, model, space, n_iter=4, scoring_fit=make_scorer(score_mse, greater_is_better=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjSWb0ldkIay"
      },
      "source": [
        "### Summarize results\r\n",
        "print(\"----- Results RandomizedSearchCV: -----\\n\" + \"Best: {} using {}\\n\".format(fitted_model.best_score_, fitted_model.best_params_))\r\n",
        "# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\r\n",
        "# pd.reset_option('all')\r\n",
        "print(\"Summary:\\n {}\".format(rs_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfBqVOpmdGJl"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZDGEMffcg0O"
      },
      "source": [
        "# ###Dropout_rate\r\n",
        "\r\n",
        "# # configure the experiment\r\n",
        "# def experiment_dropout():\r\n",
        "#   # configure the experiment\r\n",
        "#   n_dropout = [0.0, 0.2, 0.4, 0.6, 0.8]\r\n",
        "#   # run the experiment\r\n",
        "#   results = []\r\n",
        "#   for drop_value in n_dropout:\r\n",
        "#       # set dropout\r\n",
        "#       drop_out_rate = drop_value\r\n",
        "#       print(\"----- Dropout Rate: {} -----\".format(drop_out_rate))\r\n",
        "#       # evaluate\r\n",
        "#       # rather shorten code with defining a train function of code above and using it here\r\n",
        "#       vae = VAE(encoder, decoder, name=\"VAE\")\r\n",
        "#       vae.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "#       history = vae.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), verbose=0)\r\n",
        "#       # report performance\r\n",
        "#       # rathr make a dataframe or something different which is simpler to plot\r\n",
        "#       evaluation = []\r\n",
        "#       evaluation.append(vae.evaluate(x_test, y_test))\r\n",
        "#       evaluation.append(drop_value)\r\n",
        "\r\n",
        "#       res = []\r\n",
        "#       res.append(history.history[\"val_loss\"])\r\n",
        "#       print(\"val_loss = {}\".format(res))\r\n",
        "#       results.append(evaluation)\r\n",
        "#   return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfM-TqopGGUu"
      },
      "source": [
        "# results = experiment_dropout()\r\n",
        "# # summarize results\r\n",
        "# print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0e7rRTv6STo"
      },
      "source": [
        "# Visualisierung der Hyperparameter Opt. Ergebnisse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRSEFKj93Q1F"
      },
      "source": [
        "Bar Plot\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE0Ev3GX3P7o"
      },
      "source": [
        "# Scores of our Hyperparameter Optimization\r\n",
        "scores = rs_result['mean_test_score'].tolist()\r\n",
        "\r\n",
        "# Positive Score, i.e. each score in scores * (-1)\r\n",
        "posScores = []\r\n",
        "for s in scores:\r\n",
        "  posScores.append(s*(-1))\r\n",
        "\r\n",
        "indices = np.arange(0,len(scores))\r\n",
        "\r\n",
        "plt.bar(indices, posScores, tick_label=indices)\r\n",
        "plt.title(\"Score of each parameter combination\")\r\n",
        "plt.xlabel(\"Unit\")\r\n",
        "plt.ylabel(\"Score\")\r\n",
        "# save fig\r\n",
        "plt.savefig(fname = \"scores.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwY-GoamH-6e"
      },
      "source": [
        "Scatter Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr1nvfOdQ_ym"
      },
      "source": [
        "###\r\n",
        "features = ['batch_size', 'dropout_rate', 'regulazier_rate', 'learn_rate']\r\n",
        "            \r\n",
        "fig, axs = plt.subplots(nrows=len(features), ncols=len(features), figsize=(12,12))\r\n",
        "fig.tight_layout(w_pad=2, h_pad = 2)\r\n",
        "col=0\r\n",
        "row=0\r\n",
        "\r\n",
        "for feature_1 in features:\r\n",
        "  for feature_2 in features:\r\n",
        "    bestComb = min_rs_results.iloc[0:5]\r\n",
        "    rest = min_rs_results.iloc[5:]\r\n",
        "    axs[row, col].scatter(bestComb[feature_1], bestComb[feature_2], color='green', alpha=0.7)\r\n",
        "    axs[row, col].scatter(rest[feature_1], rest[feature_2], color='red', alpha=0.7)\r\n",
        "    axs[row, col].set_xlabel(feature_1, fontsize=9)\r\n",
        "    axs[row, col].set_ylabel(feature_2,  fontsize=9)\r\n",
        "    row=row+1\r\n",
        "  col=col+1\r\n",
        "  row=0\r\n",
        "\r\n",
        "# save fig\r\n",
        "plt.savefig('scoresScatter.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}