{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_v1_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nina-prog/DataAnalysis_VAE/blob/main/VAE_v2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srk9OHyCZDzL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Bidirectional, Dropout, Reshape, BatchNormalization, LeakyReLU\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcUlg_PrMiyC"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9i4M00iwYs"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcrTqGPsZceo"
      },
      "source": [
        "### Load ecg5000 data using read_csv\n",
        "ecg5000 = pd.read_csv('ECG5000_ALL.txt', sep='\\s+', header=None)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbiFe4pZlQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc5793ce-606c-4de4-be0e-68f31bac6d0f"
      },
      "source": [
        "### Optional test and info about data set\n",
        "print(\"Type of ecg5000: \\t \\t {}\".format(type(ecg5000)))\n",
        "print(\"Dimensions of ecg5000: \\t \\t {}\".format(ecg5000.shape))\n",
        "print(\"Number of elements of ecg5000: \\t {}\".format((ecg5000.size)))\n",
        "print(\"Display first 10 rows of ecg5000: \\n {}\".format(ecg5000.head(10)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of ecg5000: \t \t <class 'pandas.core.frame.DataFrame'>\n",
            "Dimensions of ecg5000: \t \t (5000, 141)\n",
            "Number of elements of ecg5000: \t 705000\n",
            "Display first 10 rows of ecg5000: \n",
            "    0         1         2         3    ...       137       138       139       140\n",
            "0  1.0 -0.112522 -2.827204 -3.773897  ...  0.228077  0.123431  0.925286  0.193137\n",
            "1  1.0 -1.100878 -3.996840 -4.285843  ...  0.476333  0.773820  1.119621 -1.436250\n",
            "2  1.0 -0.567088 -2.593450 -3.874230  ... -0.532197  0.321097  0.904227 -0.421797\n",
            "3  1.0  0.490473 -1.914407 -3.616364  ...  0.990133  1.086798  1.403011 -0.383564\n",
            "4  1.0  0.800232 -0.874252 -2.384761  ...  0.960304  0.971020  1.614392  1.421456\n",
            "5  1.0 -1.507674 -3.574550 -4.478011  ...  1.007076  1.634990  1.493366 -0.783134\n",
            "6  1.0 -0.297161 -2.766635 -4.102185  ...  0.974787  1.110407  1.288165 -0.823386\n",
            "7  1.0  0.446769 -1.507397 -3.187468  ...  1.034388  1.258433  0.961215 -0.999476\n",
            "8  1.0  0.087631 -1.753490 -3.304473  ...  0.573453  0.192971 -0.648683 -2.441068\n",
            "9  1.0 -0.832281 -1.700368 -2.257301  ...  2.126372  2.126852  1.679299  0.965814\n",
            "\n",
            "[10 rows x 141 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvRjcSqjFED"
      },
      "source": [
        "# ### Normalize dataframe with min-max-normalization to range between [-0.8, 0.8] using sklearn MinMaxScaler\n",
        "# min_max_scaler = MinMaxScaler(feature_range=(-0.8,0.8))\n",
        "# scaled_ecg5000 = pd.DataFrame(min_max_scaler.fit_transform(ecg5000))\n",
        "# print(scaled_ecg5000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Qxc-o8i2cV"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FuU0-HYavQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4dfcf5-4977-4cfe-9722-d9e690904538"
      },
      "source": [
        "### Split Data into 80/20 Training, Test\n",
        "trainDF, testDF = train_test_split(ecg5000, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Get all labels from trainDF and then drop it\n",
        "trainDF_Y = trainDF.iloc[:,0]\n",
        "trainDF.drop(trainDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# Get all labels from testDF and then drop it\n",
        "testDF_Y = testDF.iloc[:,0]\n",
        "testDF.drop(testDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# Optional test and info about new data sets\n",
        "print(\"Shape of Train DataFrame: \\t {}\".format(trainDF.shape))\n",
        "print(\"Shape of Test DataFrame: \\t {}\".format(testDF.shape))\n",
        "print(\"Shape of Train Y DataFrame: \\t {}\".format(trainDF_Y.shape))\n",
        "print(\"Shape of Test Y DataFrame: \\t {}\".format(testDF_Y.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Train DataFrame: \t (4000, 140)\n",
            "Shape of Test DataFrame: \t (1000, 140)\n",
            "Shape of Train Y DataFrame: \t (4000,)\n",
            "Shape of Test Y DataFrame: \t (1000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqDAiFlPipdc"
      },
      "source": [
        "## Reshape Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkCjqkHiHBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae8d661-130d-43a5-e67b-a78c4430de2f"
      },
      "source": [
        "### Convert to array\n",
        "x_train = trainDF.to_numpy()\n",
        "x_test = testDF.to_numpy()\n",
        "\n",
        "y_train = trainDF_Y.to_numpy()\n",
        "y_test = testDF_Y.to_numpy()\n",
        "\n",
        "### Reshape datasets X/Y train/test into [samples, timesteps, features]\n",
        "s_xtrain = len(trainDF.index) # samples\n",
        "n_xtrain = len(trainDF.columns) # time steps\n",
        "\n",
        "s_xtest = len(testDF.index) # samples\n",
        "n_xtest = len(testDF.columns) # time steps\n",
        "\n",
        "s_ytrain = len(trainDF_Y.index) # samples\n",
        "\n",
        "s_ytest = len(testDF_Y.index) # samples\n",
        "\n",
        "x_train = x_train.reshape(s_xtrain, n_xtrain, 1)\n",
        "x_test = x_test.reshape(s_xtest, n_xtest, 1)\n",
        "\n",
        "y_train = y_train.reshape(s_ytrain, 1, 1)\n",
        "y_test = y_test.reshape(s_ytest, 1, 1)\n",
        "\n",
        "### Properties\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
        "\n",
        "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
        "print(\"Shape of y_test: {}\".format(y_test.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_train: (4000, 140, 1)\n",
            "Shape of x_test: (1000, 140, 1)\n",
            "Shape of y_train: (4000, 1, 1)\n",
            "Shape of y_test: (1000, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeXLn2bUYa7g"
      },
      "source": [
        "# Create Sample Layer\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuO7o5GGYg3y"
      },
      "source": [
        "class Sampling(layers.Layer):\r\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z\"\"\"\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var = inputs\r\n",
        "        batch = tf.shape(z_mean)[0]\r\n",
        "        dim = tf.shape(z_mean)[1]\r\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F9x-KyKM4NJ"
      },
      "source": [
        "# Build Variational Autoencoder (VAE)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hcpCe0zGwQl"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4194a4eCBSh5"
      },
      "source": [
        "def create_encoder(encoding_dim=140, intermediate_dim=140, latent_dim=5, activation='tanh', dropout_rate=0.2, regulazier_rate=0.001): # maybe delete encoding and latent dim, do something with input shape\r\n",
        "    \"\"\"Maps ECG5000 time series to a triplet (z_mean, z_log_var, z).\"\"\"\r\n",
        "\r\n",
        "    ### Define Layers\r\n",
        "    encoder_inputs = keras.Input(shape=(140, 1), name='Encoder_Input_layer')\r\n",
        "\r\n",
        "    encoded = Bidirectional(LSTM(intermediate_dim, activation=activation, name=''), name='Encode_1')(encoder_inputs)\r\n",
        "    encoded = Dropout(dropout_rate, name='Dropout_1')(encoded)\r\n",
        "    encoded = Dense(latent_dim, activation=activation, name='Encode_2', kernel_regularizer=l2(regulazier_rate), activity_regularizer=l2(regulazier_rate))(encoded)\r\n",
        "\r\n",
        "    z_mean = Dense(latent_dim, activation=\"softplus\", name=\"z_mean\")(encoded)\r\n",
        "    #z_mean = BatchNormalization()(z_mean)  \r\n",
        "\r\n",
        "    z_log_var = Dense(latent_dim, activation=\"softplus\", name=\"z_log_var\")(encoded)\r\n",
        "    #z_log_var = BatchNormalization()(z_log_var)\r\n",
        "\r\n",
        "    z = Sampling(name='Sample_layer')([z_mean, z_log_var])\r\n",
        "\r\n",
        "    ### Instantiate encoder\r\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\r\n",
        "\r\n",
        "    return encoder"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCpMI6C0Diyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3be227-0fb3-4d8c-c0d2-181590cf71a0"
      },
      "source": [
        "### Check if encoder works\r\n",
        "encoder = create_encoder() \r\n",
        "encoder.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Encoder_Input_layer (InputLayer [(None, 140, 1)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encode_1 (Bidirectional)        (None, 280)          159040      Encoder_Input_layer[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "Dropout_1 (Dropout)             (None, 280)          0           Encode_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Encode_2 (Dense)                (None, 5)            1405        Dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 5)            30          Encode_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 5)            30          Encode_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Sample_layer (Sampling)         (None, 5)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 160,505\n",
            "Trainable params: 160,505\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_PrqOCOG244"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxnUeH7FBJVH"
      },
      "source": [
        "def create_decoder(encoding_dim=140, intermediate_dim=140, latent_dim=5, activation='tanh', dropout_rate=0.2, regulazier_rate=0.001): # maybe delete encoding and latent dim, do something with input shape\r\n",
        "    \"\"\"Converts z, the encoded time series, back into a readable time series.\"\"\"\r\n",
        "    \r\n",
        "    ### Define Layers\r\n",
        "    latent_inputs = keras.Input(shape=(latent_dim,), name='Decoder_Input_layer')\r\n",
        "\r\n",
        "    decoded = Dense(encoding_dim, activation=activation, name='Decode_1', kernel_regularizer=l2(regulazier_rate), activity_regularizer=l2(regulazier_rate))(latent_inputs)\r\n",
        "    decoded = Reshape((140,1), name='Decode_2')(decoded)\r\n",
        "    decoded = Dropout(dropout_rate, name='Dropout_1')(decoded)\r\n",
        "    decoded = Bidirectional(LSTM(intermediate_dim, activation=activation, return_sequences=True, name=''), name='Decode_3')(decoded)\r\n",
        "\r\n",
        "    decoder_outputs = TimeDistributed(Dense(1, activation='linear', name=''),name='Decoder_Output_Layer')(decoded)\r\n",
        "\r\n",
        "    ### Instantiate decoder\r\n",
        "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\r\n",
        "\r\n",
        "    return decoder"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHOP8O9gFoR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a9b83ed-4b01-4a98-e127-e9a3b57f3058"
      },
      "source": [
        "### Check if decoder works\r\n",
        "decoder = create_decoder()\r\n",
        "decoder.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Decoder_Input_layer (InputLa [(None, 5)]               0         \n",
            "_________________________________________________________________\n",
            "Decode_1 (Dense)             (None, 140)               840       \n",
            "_________________________________________________________________\n",
            "Decode_2 (Reshape)           (None, 140, 1)            0         \n",
            "_________________________________________________________________\n",
            "Dropout_1 (Dropout)          (None, 140, 1)            0         \n",
            "_________________________________________________________________\n",
            "Decode_3 (Bidirectional)     (None, 140, 280)          159040    \n",
            "_________________________________________________________________\n",
            "Decoder_Output_Layer (TimeDi (None, 140, 1)            281       \n",
            "=================================================================\n",
            "Total params: 160,161\n",
            "Trainable params: 160,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAKUqMR8oSm6"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNwUKMjdFM_L"
      },
      "source": [
        "Define VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9hxGlDYBBKV"
      },
      "source": [
        "# try the different definition here with the sum of reduce mean : https://keras.io/examples/generative/vae/\r\n",
        "\r\n",
        "class VAE(keras.Model):\r\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, encoder, decoder, **kwargs):\r\n",
        "        super(VAE, self).__init__(**kwargs)\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        # unpack the data\r\n",
        "        if isinstance(data, tuple):\r\n",
        "            data = data[0]\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            # forward pass\r\n",
        "            z_mean, z_log_var, z = self.encoder(data)\r\n",
        "            reconstruction = self.decoder(z)\r\n",
        "            # Compute own loss\r\n",
        "            reconstruction_loss = tf.reduce_mean(\r\n",
        "                # keras.losses.binary_crossentropy(data, reconstruction)\r\n",
        "                keras.losses.mean_squared_error(data, reconstruction)\r\n",
        "            )\r\n",
        "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\r\n",
        "            total_loss = reconstruction_loss + kl_loss\r\n",
        "        # compute gradients\r\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\r\n",
        "        # update weights\r\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\r\n",
        "        # compute own metrics\r\n",
        "        return {\r\n",
        "            \"loss\": total_loss,\r\n",
        "            \"reconstruction_loss\": reconstruction_loss,\r\n",
        "            \"kl_loss\": kl_loss,\r\n",
        "        }\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        # unpack the data\r\n",
        "        x, y = data\r\n",
        "        # compute predictions\r\n",
        "        y_pred = self(x, training=False)\r\n",
        "        # updates the metrics tracking the loss\r\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\r\n",
        "        # update the metrics\r\n",
        "        self.compiled_metrics.update_state(y, y_pred)\r\n",
        "        # return a dict mapping metric names to current value\r\n",
        "        return {m.name: m.result() for m in self.metrics}\r\n",
        "\r\n",
        "    def call(self, data):\r\n",
        "        z_mean, z_log_var, z = self.encoder(data)\r\n",
        "        reconstructed = self.decoder(z)\r\n",
        "        return reconstructed"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWJVg5WPFQlb"
      },
      "source": [
        "Build VAE connecting Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfjVoHGu5eN_"
      },
      "source": [
        "### Definde function to create model\r\n",
        "def create_model(activation='tanh', intermediate_dim=140, dropout_rate=0.2, regulazier_rate=0.004, optimizer='adam', learn_rate=0.001, name='VAE'):\r\n",
        "    \"\"\"Creates VAE model, required for wrapping in estimator interface KerasRegressor, while accepting the hyperparameters we want to tune. We also pass some default values.\"\"\"\r\n",
        "    \r\n",
        "    # create encoder \r\n",
        "    encoder = create_encoder(activation=activation, intermediate_dim=intermediate_dim, dropout_rate=dropout_rate, regulazier_rate=regulazier_rate)\r\n",
        "    # create decoder \r\n",
        "    decoder = create_decoder(activation=activation, intermediate_dim=intermediate_dim, dropout_rate=dropout_rate, regulazier_rate=regulazier_rate)\r\n",
        "    # create vae\r\n",
        "    model = VAE(encoder, decoder, name=name)\r\n",
        "    # compile model\r\n",
        "    if optimizer == 'adam':\r\n",
        "      opt = Adam(lr=learn_rate)\r\n",
        "    else:\r\n",
        "      opt = SGD(lr=learn_rate)\r\n",
        "    model.compile(optimizer=opt)\r\n",
        "    model.build((None,140,1))\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-XPM-YdEiOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20bdeab0-ad10-4954-e99e-c1fdf7fe4b39"
      },
      "source": [
        "### Instantiate VAE model\r\n",
        "vae = create_model(name='VAE')\r\n",
        "\r\n",
        "### Display VAE model and it`s parts\r\n",
        "# encoder \r\n",
        "vae.encoder.summary(line_length=100)\r\n",
        "plot_model(vae.encoder, show_shapes=True, to_file='vae_encoder.png')\r\n",
        "print(\"\\n\")\r\n",
        "# decoder\r\n",
        "vae.decoder.summary(line_length=100)\r\n",
        "plot_model(vae.decoder, show_shapes=True, to_file='vae_decoder.png')\r\n",
        "print(\"\\n\")\r\n",
        "# vae\r\n",
        "vae.summary(line_length=100)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                     Output Shape          Param #     Connected to                     \n",
            "====================================================================================================\n",
            "Encoder_Input_layer (InputLayer) [(None, 140, 1)]      0                                            \n",
            "____________________________________________________________________________________________________\n",
            "Encode_1 (Bidirectional)         (None, 280)           159040      Encoder_Input_layer[0][0]        \n",
            "____________________________________________________________________________________________________\n",
            "Dropout_1 (Dropout)              (None, 280)           0           Encode_1[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "Encode_2 (Dense)                 (None, 5)             1405        Dropout_1[0][0]                  \n",
            "____________________________________________________________________________________________________\n",
            "z_mean (Dense)                   (None, 5)             30          Encode_2[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "z_log_var (Dense)                (None, 5)             30          Encode_2[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "Sample_layer (Sampling)          (None, 5)             0           z_mean[0][0]                     \n",
            "                                                                   z_log_var[0][0]                  \n",
            "====================================================================================================\n",
            "Total params: 160,505\n",
            "Trainable params: 160,505\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Model: \"decoder\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                                 Output Shape                            Param #        \n",
            "====================================================================================================\n",
            "Decoder_Input_layer (InputLayer)             [(None, 5)]                             0              \n",
            "____________________________________________________________________________________________________\n",
            "Decode_1 (Dense)                             (None, 140)                             840            \n",
            "____________________________________________________________________________________________________\n",
            "Decode_2 (Reshape)                           (None, 140, 1)                          0              \n",
            "____________________________________________________________________________________________________\n",
            "Dropout_1 (Dropout)                          (None, 140, 1)                          0              \n",
            "____________________________________________________________________________________________________\n",
            "Decode_3 (Bidirectional)                     (None, 140, 280)                        159040         \n",
            "____________________________________________________________________________________________________\n",
            "Decoder_Output_Layer (TimeDistributed)       (None, 140, 1)                          281            \n",
            "====================================================================================================\n",
            "Total params: 160,161\n",
            "Trainable params: 160,161\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Model: \"VAE\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                                 Output Shape                            Param #        \n",
            "====================================================================================================\n",
            "encoder (Functional)                         [(None, 5), (None, 5), (None, 5)]       160505         \n",
            "____________________________________________________________________________________________________\n",
            "decoder (Functional)                         (None, 140, 1)                          160161         \n",
            "====================================================================================================\n",
            "Total params: 320,666\n",
            "Trainable params: 320,666\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7OlCIRCOUF-"
      },
      "source": [
        "# Train VAE\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf2gXbw290r5"
      },
      "source": [
        "### Train Properties\r\n",
        "epochs = 100 #50, 100\r\n",
        "batch_size = 32 #16, 32"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfnzjwIxlShE"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTkdJdc_N5SN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "0c22435b-e5cc-4d45-e0e0-94bdb9ec3caf"
      },
      "source": [
        "### To improve training see here: https://becominghuman.ai/using-variational-autoencoder-vae-to-generate-new-images-14328877e88d\r\n",
        "### Train\r\n",
        "train_history = vae.fit(x_train, x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, x_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-15652e3422d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### To improve training see here: https://becominghuman.ai/using-variational-autoencoder-vae-to-generate-new-images-14328877e88d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHZuTNLVjUeh"
      },
      "source": [
        "print(\"----- loss: -----\\n{}\".format(train_history.history[\"loss\"]))\r\n",
        "print(\"----- reconstruction_loss: -----\\n{}\".format(train_history.history[\"reconstruction_loss\"]))\r\n",
        "print(\"----- kl_loss: -----\\n{}\".format(train_history.history[\"kl_loss\"]))\r\n",
        "print(\"----- val_loss: -----\\n{}\".format(train_history.history[\"val_loss\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljaYW8HuuHZk"
      },
      "source": [
        "## Recreate Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYLbwD53cn84"
      },
      "source": [
        "# Encoder output is a list [z_mean, z_log_var, z] thus list[2] = z, see subsection encoder line 12\r\n",
        "\r\n",
        "### Extract myu i.e. z_mean\r\n",
        "z_mean = vae.encoder.predict(x_test)[0]\r\n",
        "print(\"----- z_mean: -----\")\r\n",
        "print(z_mean)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract sigma i.e. z_log_var\r\n",
        "z_log_var = vae.encoder.predict(x_test)[1]\r\n",
        "print(\"----- z_log_var: -----\")\r\n",
        "print(z_log_var)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract z_values and predict x_test\r\n",
        "z_values = vae.encoder.predict(x_test)[2]\r\n",
        "decoded_ecg5000 = vae.decoder.predict(z_values)\r\n",
        "# z_values contains list of each z_value per sample, i.e. we get 1000 SubLists with 5 elements in each.\r\n",
        "# Those 5 elements (z_values for Sample i) is our bottleneck which the decoder recieves.\r\n",
        "print(\"----- z_values: -----\")\r\n",
        "print(z_values)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Save extracted values\r\n",
        "np.savetxt('z_values.csv', z_values, delimiter=\",\")\r\n",
        "np.savetxt('decoded_ecg5000.csv', decoded_ecg5000.reshape(-1,140), delimiter=\",\")\r\n",
        "\r\n",
        "### Properties\r\n",
        "print(\"Shape of z_mean: {}\".format(z_mean.shape))\r\n",
        "print(\"Shape of z_log_var: {}\".format(z_log_var.shape))\r\n",
        "print(\"Shape of z_values: {}\".format(z_values.shape))\r\n",
        "print(\"Shape of decoded_ecg5000: {}\".format(decoded_ecg5000.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNfzqeVE2N7J"
      },
      "source": [
        "## Display the training progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ufa2OwJIVe"
      },
      "source": [
        "### Loss vs Reconstruction_loss vs KL Divergence\r\n",
        "plt.figure(figsize=(16,6))\r\n",
        "plt.plot(train_history.history['loss'])\r\n",
        "plt.plot(train_history.history['reconstruction_loss'])\r\n",
        "plt.plot(train_history.history['kl_loss'])\r\n",
        "plt.legend([\"Loss\", \"Reconstruction Loss\", \"KL Divergence\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Reconstruction Loss vs. KL Divergence\")\r\n",
        "\r\n",
        "plt.savefig('loss.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcIrsjRDYOiI"
      },
      "source": [
        "### Train loss vs val loss\r\n",
        "# Returns the loss value & metrics values for the model in test mode\r\n",
        "plt.figure(figsize=(16,6))\r\n",
        "plt.plot(train_history.history['loss'])\r\n",
        "plt.plot(train_history.history['val_loss'])\r\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Validation Loss\")\r\n",
        "\r\n",
        "plt.savefig('valLoss.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSBgUNCkM05C"
      },
      "source": [
        "#########################################################To-Do\r\n",
        "### Latent space\r\n",
        "# x,y Plot\r\n",
        "plt.figure(figsize=(12,5), linewidth=1)\r\n",
        "plt.plot(z_values[:,0], 'o')\r\n",
        "plt.plot(z_values[:,1], '+')\r\n",
        "plt.show()\r\n",
        "# Scatterplot\r\n",
        "plt.figure(figsize=(9,6))\r\n",
        "plt.scatter(z_values[:,0], z_values[:,1], s=80, c=y_test, cmap='viridis') # or cmap=hsv\r\n",
        "plt.colorbar()\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ8dyTK6kNNQ"
      },
      "source": [
        "# Plot Data Results\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfVmINR3lji"
      },
      "source": [
        "### Test if Input fits Dim of Output\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of decoded_ecg5000: {}\".format(decoded_ecg5000.shape))\n",
        "\n",
        "### Covert to 2D Array -- (\"-1\" = make a dimension (here rows) the size that will use the remaining unspecified elements)\n",
        "new_x_train= x_train.reshape(-1,140)\n",
        "new_decoded_ecg5000 = decoded_ecg5000.reshape(-1,140)\n",
        "\n",
        "print(\"Shape of Input after reshaping: {}\".format(new_x_train.shape))\n",
        "print(\"Shape of Output after reshaping: {}\".format(new_decoded_ecg5000.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG129YUrHGyU"
      },
      "source": [
        "# ### Plot for figure for paper\r\n",
        "# i = 934 # indize/sample which is going to be plotted\r\n",
        "# plt.figure(linewidth = 1, figsize=(25,6))\r\n",
        "# plt.xlabel('time steps')\r\n",
        "# plt.plot(new_x_train[i])\r\n",
        "# plt.show()\r\n",
        "# plt.savefig('diagramm_original.jpg')\r\n",
        "\r\n",
        "# plt.figure(linewidth = 1, figsize=(25,6))\r\n",
        "# plt.xlabel('time steps')\r\n",
        "# plt.plot(new_decoded_ecg5000[i], label='decoded ecg5000')\r\n",
        "# plt.show()\r\n",
        "# plt.savefig('diagramm_decoded.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGvzjGwcJi1B"
      },
      "source": [
        "### Plot only one sample\r\n",
        "i = 934 # indize/sample which is going to be plotted\r\n",
        "plt.figure(linewidth = 1, figsize=(20,6))\r\n",
        "plt.title('Autoencoder Result')\r\n",
        "plt.xlabel('time steps')\r\n",
        "plt.plot(new_x_train[i], label='original ecg5000')\r\n",
        "plt.plot(new_decoded_ecg5000[i], label='decoded ecg5000')\r\n",
        "plt.legend(loc=\"upper left\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjUjaCtkSpW"
      },
      "source": [
        "### Plot Multiple Samples\n",
        "n_rows = 2                   \n",
        "n_cols = 3\n",
        "\n",
        "# Size Properties and layout design for tighter representation\n",
        "fig, axs = plt.subplots(nrows = n_rows, ncols = n_cols, figsize=(20,7))\n",
        "fig.tight_layout(w_pad=4, h_pad = 5)\n",
        "\n",
        "# Subplotting\n",
        "i = 50\n",
        "for row in range(n_rows):\n",
        "  for col in range(n_cols):\n",
        "    axs[row, col].plot(new_x_train[i])\n",
        "    axs[row, col].plot(new_decoded_ecg5000[i])\n",
        "    axs[row, col].legend([\"Original ECG5000 Sample {}\".format(i), \"Decoded ECG5000 Sample {}\".format(i)])\n",
        "    axs[row, col].set(xlabel = \"Time Steps\", ylabel = \"Heartbeat Interpolated\", title = \"Sample {}\".format(i))\n",
        "    i = i + 75\n",
        "\n",
        "plt.savefig('dataComparison.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unWrNuYBc71O"
      },
      "source": [
        "# Optimization\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX8k4aeUCx3k"
      },
      "source": [
        "## Hyperparameter (Sckit_GridSearchCV)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywVsad-OqE7a"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "from sklearn.metrics import mean_squared_error, make_scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3NsIDD4dWHN"
      },
      "source": [
        "### Define scorer\r\n",
        "def score_mse(y_true, y_pred): # , **kwargs\r\n",
        "    \"\"\"Implementing mean squarred error as a score for RandomizedSearchCV.\"\"\"\r\n",
        "\r\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\r\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\r\n",
        "    # removing all size 1 dimensions in y_true\r\n",
        "    y_true = tf.squeeze(y_true)\r\n",
        "    return np.mean(tf.math.squared_difference(y_pred, y_true))\r\n",
        "\r\n",
        "### Define Function to randomizedSearch \r\n",
        "def randomizedSearch_pipeline(x_train_data, x_test_data, model, space, n_iter=10, scoring_fit='neg_mean_squared_error', cv=5, do_probabilities = False):\r\n",
        "    \"\"\"Pipeline for RandomizedSearchCV: Select settings and run randomizedSearchCV, returning results.\"\"\"\r\n",
        "    # define randomizedSearch\r\n",
        "    rs = RandomizedSearchCV(\r\n",
        "        estimator=model,\r\n",
        "        param_distributions=space, \r\n",
        "        n_iter=n_iter,\r\n",
        "        scoring=scoring_fit,\r\n",
        "        n_jobs=1, \r\n",
        "        cv=cv, \r\n",
        "        verbose=2, \r\n",
        "        random_state=1,\r\n",
        "    )\r\n",
        "    # fit model\r\n",
        "    fitted_model = rs.fit(x_train_data, x_train_data, verbose=0)\r\n",
        "    # get results\r\n",
        "    rs_result = pd.DataFrame(rs.cv_results_)\r\n",
        "    # save compromised version of the results\r\n",
        "    min_rs_results = pd.concat([pd.DataFrame(rs.cv_results_[\"mean_test_score\"], columns=[\"Score\"]),\r\n",
        "                                pd.DataFrame(rs.cv_results_[\"params\"])], axis=1)\r\n",
        "    min_rs_results.to_latex('randomizedSearchResults.tex')\r\n",
        "    \r\n",
        "    if do_probabilities:\r\n",
        "      pred = fitted_model.predict_proba(x_test_data)\r\n",
        "    else:\r\n",
        "      pred = fitted_model.predict(x_test_data)\r\n",
        "    \r\n",
        "    return fitted_model, pred, rs_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL_QfMgwCxfi"
      },
      "source": [
        "### Define evaluated params and it's value range\r\n",
        "space = {\r\n",
        "        #'epochs' :              [20, 50],\r\n",
        "        #'batch_size' :          [16, 32],\r\n",
        "        #'activation' :          ['tanh'],\r\n",
        "        'dropout_rate' :        [0.2, 0.3, 0.5],\r\n",
        "        'regulazier_rate' :     [0.001,0.004],\r\n",
        "        'optimizer' :           ['adam', 'SGD'],\r\n",
        "        'learn_rate' :          [0.001, 0.01],\r\n",
        "        }\r\n",
        "\r\n",
        "### Wrap keras custom VAE model with the KerasClassifier thus it implements estimator interface\r\n",
        "model = KerasRegressor(build_fn=create_model)\r\n",
        "\r\n",
        "### Run RandomizedSearch\r\n",
        "#m=make_scorer(mean_squared, greater_is_better=False)\r\n",
        "fitted_model, pred, rs_result = randomizedSearch_pipeline(x_train, x_test, model, space, n_iter=20, scoring_fit=make_scorer(score_mse, greater_is_better=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjSWb0ldkIay"
      },
      "source": [
        "### Summarize results\r\n",
        "print(\"----- Results RandomizedSearchCV: -----\\n\" + \"Best: {} using {}\\n\".format(fitted_model.best_score_, fitted_model.best_params_))\r\n",
        "# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\r\n",
        "# pd.reset_option('all')\r\n",
        "print(\"Summary:\\n {}\".format(rs_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfBqVOpmdGJl"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZDGEMffcg0O"
      },
      "source": [
        "###Dropout_rate\r\n",
        "\r\n",
        "# configure the experiment\r\n",
        "def experiment_dropout():\r\n",
        "  # configure the experiment\r\n",
        "  n_dropout = [0.0, 0.2, 0.4, 0.6, 0.8]\r\n",
        "  # run the experiment\r\n",
        "  results = []\r\n",
        "  for drop_value in n_dropout:\r\n",
        "      # set dropout\r\n",
        "      drop_out_rate = drop_value\r\n",
        "      print(\"----- Dropout Rate: {} -----\".format(drop_out_rate))\r\n",
        "      # evaluate\r\n",
        "      # rather shorten code with defining a train function of code above and using it here\r\n",
        "      vae = VAE(encoder, decoder, name=\"VAE\")\r\n",
        "      vae.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "      history = vae.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), verbose=0)\r\n",
        "      # report performance\r\n",
        "      # rathr make a dataframe or something different which is simpler to plot\r\n",
        "      evaluation = []\r\n",
        "      evaluation.append(vae.evaluate(x_test, y_test))\r\n",
        "      evaluation.append(drop_value)\r\n",
        "\r\n",
        "      res = []\r\n",
        "      res.append(history.history[\"val_loss\"])\r\n",
        "      print(\"val_loss = {}\".format(res))\r\n",
        "      results.append(evaluation)\r\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfM-TqopGGUu"
      },
      "source": [
        "results = experiment_dropout()\r\n",
        "# summarize results\r\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}