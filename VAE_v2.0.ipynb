{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_v1_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nina-prog/DataAnalysis_VAE/blob/main/VAE_v2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srk9OHyCZDzL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcUlg_PrMiyC"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9i4M00iwYs"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcrTqGPsZceo"
      },
      "source": [
        "### Load ecg5000 data using read_csv\n",
        "ecg5000 = pd.read_csv('ECG5000_ALL.txt', sep='\\s+', header=None)\n",
        "\n",
        "### Delete label-column first (column 0)\n",
        "ecg5000.drop(ecg5000.columns[[0]], axis=1, inplace=True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbiFe4pZlQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d938ce7-043e-4463-e66d-dede25006118"
      },
      "source": [
        "### Optional test and info about data set\n",
        "print(\"Type of ecg5000: \\t \\t {}\".format(type(ecg5000)))\n",
        "print(\"Dimensions of ecg5000: \\t \\t {}\".format(ecg5000.shape))\n",
        "print(\"Number of elements of ecg5000: \\t {}\".format((ecg5000.size)))\n",
        "print(\"Display first 10 rows of ecg5000: \\n {}\".format(ecg5000.head(10)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of ecg5000: \t \t <class 'pandas.core.frame.DataFrame'>\n",
            "Dimensions of ecg5000: \t \t (5000, 140)\n",
            "Number of elements of ecg5000: \t 700000\n",
            "Display first 10 rows of ecg5000: \n",
            "         1         2         3    ...       138       139       140\n",
            "0 -0.112522 -2.827204 -3.773897  ...  0.123431  0.925286  0.193137\n",
            "1 -1.100878 -3.996840 -4.285843  ...  0.773820  1.119621 -1.436250\n",
            "2 -0.567088 -2.593450 -3.874230  ...  0.321097  0.904227 -0.421797\n",
            "3  0.490473 -1.914407 -3.616364  ...  1.086798  1.403011 -0.383564\n",
            "4  0.800232 -0.874252 -2.384761  ...  0.971020  1.614392  1.421456\n",
            "5 -1.507674 -3.574550 -4.478011  ...  1.634990  1.493366 -0.783134\n",
            "6 -0.297161 -2.766635 -4.102185  ...  1.110407  1.288165 -0.823386\n",
            "7  0.446769 -1.507397 -3.187468  ...  1.258433  0.961215 -0.999476\n",
            "8  0.087631 -1.753490 -3.304473  ...  0.192971 -0.648683 -2.441068\n",
            "9 -0.832281 -1.700368 -2.257301  ...  2.126852  1.679299  0.965814\n",
            "\n",
            "[10 rows x 140 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOl-yE1aizs4"
      },
      "source": [
        "## Scale Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvRjcSqjFED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16befc93-378f-4e87-b9d8-c89ac38e180a"
      },
      "source": [
        "### Normalize dataframe with min-max-normalization to range between [-0.8, 0.8] using sklearn MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-0.8,0.8))\n",
        "scaled_ecg5000 = pd.DataFrame(min_max_scaler.fit_transform(ecg5000))\n",
        "print(scaled_ecg5000)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           0         1         2    ...       137       138       139\n",
            "0     0.105202 -0.154680 -0.521072  ...  0.190712  0.256566 -0.054755\n",
            "1    -0.030005 -0.331729 -0.626180  ...  0.305374  0.286734 -0.247949\n",
            "2     0.043017 -0.119297 -0.541672  ...  0.225560  0.253297 -0.127667\n",
            "3     0.187692 -0.016509 -0.488729  ...  0.360551  0.330726 -0.123133\n",
            "4     0.230067  0.140940 -0.235867  ...  0.340140  0.363540  0.090885\n",
            "...        ...       ...       ...  ...       ...       ...       ...\n",
            "4995 -0.033027 -0.067751 -0.335005  ... -0.337752 -0.198844 -0.292066\n",
            "4996  0.045669 -0.012746 -0.329288  ...  0.391322  0.291519  0.017785\n",
            "4997 -0.064328 -0.061103 -0.263680  ... -0.229486 -0.132008 -0.158819\n",
            "4998 -0.033227 -0.015091 -0.196437  ...  0.090797  0.026031 -0.064782\n",
            "4999  0.220296  0.302430  0.103076  ...  0.440269  0.378969  0.077596\n",
            "\n",
            "[5000 rows x 140 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Qxc-o8i2cV"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FuU0-HYavQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726d5399-1d53-4ad3-e20e-1a9985104cc0"
      },
      "source": [
        "### Split Data into 80/20 Training, Test\n",
        "trainDF, testDF = train_test_split(scaled_ecg5000, test_size=0.2)\n",
        "# Optional test and info about new data sets\n",
        "print(\"Shape of Training DataFrame: \\t {}\".format(trainDF.shape))\n",
        "print(\"Shape of Test DataFrame: \\t {}\".format(testDF.shape))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Training DataFrame: \t (4000, 140)\n",
            "Shape of Test DataFrame: \t (1000, 140)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqDAiFlPipdc"
      },
      "source": [
        "## Reshape Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkCjqkHiHBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459aacb9-36bd-4509-b340-d1dd7ceccbb7"
      },
      "source": [
        "### Convert to array\n",
        "x_train = trainDF.to_numpy()\n",
        "x_test = testDF.to_numpy()\n",
        "\n",
        "### Reshape input into [samples, timesteps, features]\n",
        "s_train = len(trainDF.index) # samples\n",
        "s_test = len(testDF.index) # samples\n",
        "n_train = len(trainDF.columns) # time steps\n",
        "n_test = len(testDF.columns) # time steps\n",
        "x_train = x_train.reshape(s_train, n_train, 1)\n",
        "x_test = x_test.reshape(s_test, n_test, 1)\n",
        "\n",
        "### Properties\n",
        "print(\"Shape of reshaped train dataset: {}\".format(x_train.shape))\n",
        "print(\"Shape of reshaped test dataset: {}\".format(x_test.shape))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of reshaped train dataset: (4000, 140, 1)\n",
            "Shape of reshaped test dataset: (1000, 140, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeXLn2bUYa7g"
      },
      "source": [
        "# Create Sample Layer\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuO7o5GGYg3y"
      },
      "source": [
        "class Sampling(layers.Layer):\r\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z\"\"\"\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var = inputs\r\n",
        "        batch = tf.shape(z_mean)[0]\r\n",
        "        dim = tf.shape(z_mean)[1]\r\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F9x-KyKM4NJ"
      },
      "source": [
        "# Build Variational Autoencoder (VAE)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmlHrviGtyZ"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_pSJx0oFlvB"
      },
      "source": [
        "### For better understanding visit: https://towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e\r\n",
        "### For better understanding of layers and Recreating auto encoders visit: https://machinelearningmastery.com/lstm-autoencoders/\r\n",
        "### or for code: https://gist.github.com/GerardBCN/40349b39bc45d4550141aff6966d1619#file-stock_price_autoencoding-ipynb\r\n",
        "### For Reshaping Issues: https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\r\n",
        "\r\n",
        "### fit model\r\n",
        "encoding_dim = 140\r\n",
        "latent_dim = 2\r\n",
        "epochs = 50\r\n",
        "batch_size = 32"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hcpCe0zGwQl"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4194a4eCBSh5"
      },
      "source": [
        "### Define Encoder Layers\r\n",
        "encoder_inputs = keras.Input(shape=(140, 1), name='Encoder_Input_layer')\r\n",
        "\r\n",
        "encoded = Bidirectional(layers.LSTM(encoding_dim, activation='tanh', name=''), name='Encode_1')(encoder_inputs)\r\n",
        "encoded = layers.Dense(5, activation='tanh', name='Encode_2')(encoded) #5, because 5 class in data ecg5000 - evtl 2,1\r\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(encoded) \r\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(encoded)\r\n",
        "\r\n",
        "z = Sampling(name='Sample_layer')([z_mean, z_log_var])\r\n",
        "\r\n",
        "### Build Encoder\r\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\r\n",
        "\r\n",
        "encoder.summary()\r\n",
        "plot_model(encoder, show_shapes=True, to_file='reconstruct_lstm_encoder.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_PrqOCOG244"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxnUeH7FBJVH"
      },
      "source": [
        "### Define Decoder Layers\r\n",
        "latent_inputs = keras.Input(shape=(latent_dim,), name='Decoder_Input_layer')\r\n",
        "\r\n",
        "decoded = layers.Dense(140, activation='tanh', name='Decode_1')(latent_inputs)\r\n",
        "decoded = layers.Reshape((140,1), name='Decode_2')(decoded)\r\n",
        "decoded = Bidirectional(layers.LSTM(encoding_dim, return_sequences=True, activation='tanh', name=''), name='Decode_3')(decoded)\r\n",
        "\r\n",
        "decoder_outputs = TimeDistributed(Dense(1, activation='tanh', name=''),name='Decoder_Output_Layer')(decoded)\r\n",
        "\r\n",
        "### Build Decoder\r\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\r\n",
        "\r\n",
        "decoder.summary()\r\n",
        "plot_model(decoder, show_shapes=True, to_file='reconstruct_lstm_decoder.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAKUqMR8oSm6"
      },
      "source": [
        "### Connecting the Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR-wmYsdQXII"
      },
      "source": [
        "### For better understanding visit https://becominghuman.ai/using-variational-autoencoder-vae-to-generate-new-images-14328877e88d\r\n",
        "\r\n",
        "### Instantiate VAE model\r\n",
        "# The output of vae model is the output of decoder in which its input is taken from the output of encoder !\r\n",
        "decoder_outputs = decoder(encoder(encoder_inputs))\r\n",
        "vae = keras.Model(encoder_inputs, decoder_outputs, name='vae_v2_0')\r\n",
        "\r\n",
        "vae.summary()\r\n",
        "plot_model(vae, show_shapes=True, to_file='reconstruct_lstm_variational_autoencoder.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7OlCIRCOUF-"
      },
      "source": [
        "# Train VAE\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1m0bKlZlIuL"
      },
      "source": [
        "## Define Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9hxGlDYBBKV"
      },
      "source": [
        "class VAE(keras.Model):\r\n",
        "    def __init__(self, encoder, decoder, **kwargs):\r\n",
        "        super(VAE, self).__init__(**kwargs)\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        if isinstance(data, tuple):\r\n",
        "            data = data[0]\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            z_mean, z_log_var, z = encoder(data)\r\n",
        "            reconstruction = decoder(z)\r\n",
        "            reconstruction_loss = tf.reduce_mean(\r\n",
        "                keras.losses.binary_crossentropy(data, reconstruction)\r\n",
        "            )\r\n",
        "            reconstruction_loss *= 28 * 28\r\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\r\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\r\n",
        "            kl_loss *= -0.5\r\n",
        "            total_loss = reconstruction_loss + kl_loss\r\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\r\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\r\n",
        "        return {\r\n",
        "            \"loss\": total_loss,\r\n",
        "            \"reconstruction_loss\": reconstruction_loss,\r\n",
        "            \"kl_loss\": kl_loss,\r\n",
        "        }"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfnzjwIxlShE"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTkdJdc_N5SN"
      },
      "source": [
        "### To improve training see here: https://becominghuman.ai/using-variational-autoencoder-vae-to-generate-new-images-14328877e88d\r\n",
        " \r\n",
        "### Train\r\n",
        "vae = VAE(encoder, decoder, name=\"VAE\")\r\n",
        "vae.compile(optimizer='adam', loss='binary_crossentropy')\r\n",
        "history = vae.fit(x_train, epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ufa2OwJIVe"
      },
      "source": [
        "### Display the training progress\r\n",
        "# WHAT IS THE DIFFERENCE BETWEEN THESE LOSS`s? WHAT DO THEY SHOW? WHAT ARE THEY?\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['reconstruction_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-4uxGq4PgAv"
      },
      "source": [
        "#### TODO (2): GET LATENT VECTOR AND SHOW MYU AND SIGMA\r\n",
        "print(z_log_var)\r\n",
        "print(z_mean)\r\n",
        "\r\n",
        "### Recreation\r\n",
        "bottleneck = encoder.predict(x_train)\r\n",
        "decoded_ecg5000 = decoder.predict(bottleneck)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOXeC5qZgBai"
      },
      "source": [
        "### TODO (3): Display latent space\r\n",
        "### Encode data into latent space and show the distribution using simple scatter plot\r\n",
        "bottleneck = encoder.predict(x_train)\r\n",
        "\r\n",
        "print(bottleneck)\r\n",
        "print(type(bottleneck))\r\n",
        "print(len(bottleneck))\r\n",
        "\r\n",
        "array0 = bottleneck[0]\r\n",
        "print(len(array0))\r\n",
        "print(array0.shape)\r\n",
        "array1 = bottleneck[1]\r\n",
        "print(len(array1))\r\n",
        "print(array1.shape)\r\n",
        "array2 = bottleneck[2]\r\n",
        "print(len(array2))\r\n",
        "print(array2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSBgUNCkM05C"
      },
      "source": [
        "# *i unpacks i into a tuple (i[0], i[1]), which is interpreted as (x,y) by plt.scatter\r\n",
        "# for a in bottleneck:\r\n",
        "#  for i in a:\r\n",
        "#   plt.scatter(*i)\r\n",
        "\r\n",
        "#x = array1[:,0]\r\n",
        "#y = array1[:,1]\r\n",
        "\r\n",
        "plt.figure(figsize=(18,12))\r\n",
        "plt.scatter(array0[:,0], array0[:,1], s=2)\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ8dyTK6kNNQ"
      },
      "source": [
        "# Plot Results\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfVmINR3lji"
      },
      "source": [
        "### Test if Input fits Dim of Output\n",
        "print(\"Shape of Input x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of Output y_train: {}\".format(decoded_ecg5000.shape))\n",
        "\n",
        "### Covert to 2D DataFrame -- (\"-1\" = selecting all data)\n",
        "new_x_train= x_train.reshape(-1,140)\n",
        "new_decoded_ecg5000 = decoded_ecg5000.reshape(-1,140)\n",
        "\n",
        "print(\"Shape of Input after reshaping: {}\".format(new_x_train.shape))\n",
        "print(\"Shape of Output after reshaping: {}\".format(new_decoded_ecg5000.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG129YUrHGyU"
      },
      "source": [
        "### Plot only one sample\r\n",
        "i = 34 # indize/sample which is going to be plotted\r\n",
        "plt.figure(linewidth = 1, figsize=(25,6))\r\n",
        "plt.title('Autoencoder Result')\r\n",
        "plt.xlabel('time steps')\r\n",
        "plt.plot(new_x_train[i], label='original ecg5000')\r\n",
        "plt.plot(new_decoded_ecg5000[i], label='decoded ecg5000')\r\n",
        "plt.legend(loc=\"upper left\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjUjaCtkSpW"
      },
      "source": [
        "### Plot Multiple Samples\n",
        "n_rows = 3                   \n",
        "n_cols = 2\n",
        "\n",
        "# Size Properties and layout design for tighter representation\n",
        "fig, axs = plt.subplots(nrows = n_rows, ncols = n_cols, figsize=(23,20))\n",
        "fig.tight_layout(w_pad=4, h_pad = 5)\n",
        "\n",
        "# Subplotting\n",
        "i = 0\n",
        "for row in range(n_rows):\n",
        "  for col in range(n_cols):\n",
        "    axs[row, col].plot(new_x_train[i])\n",
        "    axs[row, col].plot(new_decoded_ecg5000[i])\n",
        "    axs[row, col].legend([\"Original ECG5000 Sample {}\".format(i), \"Decoded ECG5000 Sample {}\".format(i)])\n",
        "    axs[row, col].set(xlabel = \"Time Steps\", ylabel = \"Heartbet Interpolated\", title = \"Sample {}\".format(i))\n",
        "    i = i + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}