{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_v1_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nina-prog/DataAnalysis_VAE/blob/main/VAE_v2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srk9OHyCZDzL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Bidirectional, Dropout, Reshape, Flatten\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "### set plot design\n",
        "sns.set()\n",
        "sns.set_palette(sns.color_palette(\"Set1\"))  # tab10 #viridis\n",
        "# sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"paper\")\n",
        "\n",
        "### set seed\n",
        "tf.random.set_seed(7)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcUlg_PrMiyC"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9i4M00iwYs"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcrTqGPsZceo"
      },
      "source": [
        "### Load ecg5000 data using read_csv\n",
        "ecg5000 = pd.read_csv('ECG5000_ALL.txt', sep='\\s+', header=None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbiFe4pZlQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42748351-f7f7-4457-f4ad-2c1fd79e1466"
      },
      "source": [
        "### Optional test and info about data set\n",
        "print(\"Type of ecg5000: \\t \\t {}\".format(type(ecg5000)))\n",
        "print(\"Dimensions of ecg5000: \\t \\t {}\".format(ecg5000.shape))\n",
        "print(\"Number of elements of ecg5000: \\t {}\".format((ecg5000.size)))\n",
        "print(\"Display first 10 rows of ecg5000: \\n {}\".format(ecg5000.head(10)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of ecg5000: \t \t <class 'pandas.core.frame.DataFrame'>\n",
            "Dimensions of ecg5000: \t \t (5000, 141)\n",
            "Number of elements of ecg5000: \t 705000\n",
            "Display first 10 rows of ecg5000: \n",
            "    0         1         2         3    ...       137       138       139       140\n",
            "0  1.0 -0.112522 -2.827204 -3.773897  ...  0.228077  0.123431  0.925286  0.193137\n",
            "1  1.0 -1.100878 -3.996840 -4.285843  ...  0.476333  0.773820  1.119621 -1.436250\n",
            "2  1.0 -0.567088 -2.593450 -3.874230  ... -0.532197  0.321097  0.904227 -0.421797\n",
            "3  1.0  0.490473 -1.914407 -3.616364  ...  0.990133  1.086798  1.403011 -0.383564\n",
            "4  1.0  0.800232 -0.874252 -2.384761  ...  0.960304  0.971020  1.614392  1.421456\n",
            "5  1.0 -1.507674 -3.574550 -4.478011  ...  1.007076  1.634990  1.493366 -0.783134\n",
            "6  1.0 -0.297161 -2.766635 -4.102185  ...  0.974787  1.110407  1.288165 -0.823386\n",
            "7  1.0  0.446769 -1.507397 -3.187468  ...  1.034388  1.258433  0.961215 -0.999476\n",
            "8  1.0  0.087631 -1.753490 -3.304473  ...  0.573453  0.192971 -0.648683 -2.441068\n",
            "9  1.0 -0.832281 -1.700368 -2.257301  ...  2.126372  2.126852  1.679299  0.965814\n",
            "\n",
            "[10 rows x 141 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvRjcSqjFED"
      },
      "source": [
        "# ### Normalize dataframe with min-max-normalization to range between [-0.8, 0.8] using sklearn MinMaxScaler\n",
        "# min_max_scaler = MinMaxScaler(feature_range=(-0.8,0.8))\n",
        "# scaled_ecg5000 = pd.DataFrame(min_max_scaler.fit_transform(ecg5000))\n",
        "# print(scaled_ecg5000)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Qxc-o8i2cV"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FuU0-HYavQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be1283c-cb9a-4897-b115-a58328290b0b"
      },
      "source": [
        "### Split Data into 80/20 Training, Test\n",
        "trainDF, testDF = train_test_split(ecg5000, test_size=0.2, shuffle=True, random_state=1)\n",
        "\n",
        "# get all labels from trainDF and then drop it\n",
        "trainDF_Y = trainDF.iloc[:,0]\n",
        "trainDF.drop(trainDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# get all labels from testDF and then drop it\n",
        "testDF_Y = testDF.iloc[:,0]\n",
        "testDF.drop(testDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# optional test and info about new data sets\n",
        "print(\"Shape of Train DataFrame: \\t {}\".format(trainDF.shape))\n",
        "print(\"Shape of Test DataFrame: \\t {}\".format(testDF.shape))\n",
        "print(\"Shape of Train Y DataFrame: \\t {}\".format(trainDF_Y.shape))\n",
        "print(\"Shape of Test Y DataFrame: \\t {}\".format(testDF_Y.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Train DataFrame: \t (4000, 140)\n",
            "Shape of Test DataFrame: \t (1000, 140)\n",
            "Shape of Train Y DataFrame: \t (4000,)\n",
            "Shape of Test Y DataFrame: \t (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqDAiFlPipdc"
      },
      "source": [
        "## Reshape Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkCjqkHiHBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac077801-8f86-42f9-f5ff-86979dab1d6e"
      },
      "source": [
        "### Convert to array\n",
        "x_train = trainDF.to_numpy()\n",
        "x_test = testDF.to_numpy()\n",
        "\n",
        "y_train = trainDF_Y.to_numpy()\n",
        "y_test = testDF_Y.to_numpy()\n",
        "\n",
        "### Reshape datasets X/Y train/test into [samples, time steps, features]\n",
        "s_x_train = len(trainDF.index)  # samples\n",
        "n_x_train = len(trainDF.columns)  # time steps\n",
        "\n",
        "s_x_test = len(testDF.index)  # samples\n",
        "n_x_test = len(testDF.columns)  # time steps\n",
        "\n",
        "s_y_train = len(trainDF_Y.index)  # samples\n",
        "\n",
        "s_y_test = len(testDF_Y.index)  # samples\n",
        "\n",
        "x_train = x_train.reshape(s_x_train, n_x_train, 1)\n",
        "x_test = x_test.reshape(s_x_test, n_x_test, 1)\n",
        "\n",
        "y_train = y_train.reshape(s_y_train, 1, 1)\n",
        "y_test = y_test.reshape(s_y_test, 1, 1)\n",
        "\n",
        "### Properties\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
        "\n",
        "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
        "print(\"Shape of y_test: {}\".format(y_test.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_train: (4000, 140, 1)\n",
            "Shape of x_test: (1000, 140, 1)\n",
            "Shape of y_train: (4000, 1, 1)\n",
            "Shape of y_test: (1000, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeXLn2bUYa7g"
      },
      "source": [
        "# Sampling\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuO7o5GGYg3y"
      },
      "source": [
        "class Sampling(layers.Layer):\r\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z\"\"\"\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var = inputs\r\n",
        "        batch = tf.shape(z_mean)[0]\r\n",
        "        dim = tf.shape(z_mean)[1]\r\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F9x-KyKM4NJ"
      },
      "source": [
        "# Build Variational Autoencoder (VAE)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hcpCe0zGwQl"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4194a4eCBSh5"
      },
      "source": [
        "def create_encoder(intermediate_dim=140, latent_dim=5, dropout_rate=0.2, regularizer_rate=0.004):\r\n",
        "    \"\"\"Maps ECG5000 time series to a triplet (z_mean, z_log_var, z).\"\"\"\r\n",
        "\r\n",
        "    ### Define Layers\r\n",
        "    encoder_inputs = keras.Input(shape=(140, 1), name='Encoder_Input_layer')\r\n",
        "\r\n",
        "    encoded = Bidirectional(LSTM(intermediate_dim, activation='tanh', name=''), name='Encode_1')(encoder_inputs)\r\n",
        "    # encoded = Flatten()(encoded), LSTM return_sequence=True\r\n",
        "    encoded = Dropout(dropout_rate, name='Dropout_1')(encoded)\r\n",
        "    encoded = Dense(latent_dim, activation='tanh', name='Encode_2', kernel_regularizer=l2(regularizer_rate),\r\n",
        "                    activity_regularizer=l2(regularizer_rate))(encoded)\r\n",
        "\r\n",
        "    z_mean = Dense(latent_dim, activation='softplus', name=\"z_mean\")(encoded)\r\n",
        "    z_log_var = Dense(latent_dim, activation='softplus', name=\"z_log_var\")(encoded)\r\n",
        "    z = Sampling(name='Sample_layer')([z_mean, z_log_var])\r\n",
        "\r\n",
        "    ### Instantiate encoder\r\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\r\n",
        "\r\n",
        "    return encoder"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCpMI6C0Diyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f27945-26a7-4923-8bd1-d3e00a3a2d23"
      },
      "source": [
        "### Check if encoder works\r\n",
        "encoder_test = create_encoder() \r\n",
        "encoder_test.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Encoder_Input_layer (InputLayer [(None, 140, 1)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encode_1 (Bidirectional)        (None, 280)          159040      Encoder_Input_layer[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "Dropout_1 (Dropout)             (None, 280)          0           Encode_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Encode_2 (Dense)                (None, 5)            1405        Dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 5)            30          Encode_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 5)            30          Encode_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Sample_layer (Sampling)         (None, 5)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 160,505\n",
            "Trainable params: 160,505\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_PrqOCOG244"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxnUeH7FBJVH"
      },
      "source": [
        "def create_decoder(encoding_dim=140, intermediate_dim=140, latent_dim=5, dropout_rate=0.2,\r\n",
        "                   regularizer_rate=0.004):\r\n",
        "    \"\"\"Converts z, the encoded time series, back into a readable time series.\"\"\"\r\n",
        "\r\n",
        "    ### Define Layers\r\n",
        "    latent_inputs = keras.Input(shape=(latent_dim,), name='Decoder_Input_layer')\r\n",
        "\r\n",
        "    decoded = Dense(encoding_dim * 256, activation='tanh', name='Decode_1', kernel_regularizer=l2(regularizer_rate),\r\n",
        "                    activity_regularizer=l2(regularizer_rate))(latent_inputs)\r\n",
        "    decoded = Reshape((140, 256), name='Decode_2')(decoded)\r\n",
        "    decoded = Dropout(dropout_rate, name='Dropout_1')(decoded)\r\n",
        "    decoded = Bidirectional(LSTM(intermediate_dim, activation='tanh', return_sequences=True, name=''), name='Decode_3')(\r\n",
        "         decoded)\r\n",
        "\r\n",
        "    decoder_outputs = TimeDistributed(Dense(1, activation='linear', name=''), name='Decoder_Output_Layer')(decoded)\r\n",
        "\r\n",
        "    ### Instantiate decoder\r\n",
        "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\r\n",
        "\r\n",
        "    return decoder\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHOP8O9gFoR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9063610a-aeae-4881-c0d2-116d1a8eab18"
      },
      "source": [
        "### Check if decoder works\r\n",
        "decoder_test = create_decoder()\r\n",
        "decoder_test.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Decoder_Input_layer (InputLa [(None, 5)]               0         \n",
            "_________________________________________________________________\n",
            "Decode_1 (Dense)             (None, 35840)             215040    \n",
            "_________________________________________________________________\n",
            "Decode_2 (Reshape)           (None, 140, 256)          0         \n",
            "_________________________________________________________________\n",
            "Dropout_1 (Dropout)          (None, 140, 256)          0         \n",
            "_________________________________________________________________\n",
            "Decode_3 (Bidirectional)     (None, 140, 280)          444640    \n",
            "_________________________________________________________________\n",
            "Decoder_Output_Layer (TimeDi (None, 140, 1)            281       \n",
            "=================================================================\n",
            "Total params: 659,961\n",
            "Trainable params: 659,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAKUqMR8oSm6"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNwUKMjdFM_L"
      },
      "source": [
        "### Define VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9hxGlDYBBKV"
      },
      "source": [
        "class VAE(keras.Model):\r\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, encoder, decoder, **kwargs):\r\n",
        "        super(VAE, self).__init__(**kwargs)\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        # unpack the data\r\n",
        "        if isinstance(data, tuple):\r\n",
        "            data = data[0]\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            # forward pass\r\n",
        "            z_mean, z_log_var, z = self.encoder(data)\r\n",
        "            reconstruction = self.decoder(z)\r\n",
        "            # Compute own loss\r\n",
        "            reconstruction_loss = tf.reduce_mean(\r\n",
        "                keras.losses.mean_squared_error(data, reconstruction) * 140\r\n",
        "            )\r\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\r\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\r\n",
        "            kl_loss *= -0.5\r\n",
        "            total_loss = reconstruction_loss + kl_loss\r\n",
        "        # compute gradients\r\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\r\n",
        "        # update weights\r\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\r\n",
        "        # compute own metrics\r\n",
        "        return {\r\n",
        "            \"loss\": total_loss,\r\n",
        "            \"reconstruction_loss\": reconstruction_loss,\r\n",
        "            \"kl_loss\": kl_loss,\r\n",
        "        }\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        # unpack the data\r\n",
        "        x, y = data\r\n",
        "        # compute predictions\r\n",
        "        y_pred = self(x, training=False)\r\n",
        "        # updates the metrics tracking the loss\r\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\r\n",
        "        # update the metrics\r\n",
        "        self.compiled_metrics.update_state(y, y_pred)\r\n",
        "        # return a dict mapping metric names to current value\r\n",
        "        return {m.name: m.result() for m in self.metrics}\r\n",
        "\r\n",
        "    def call(self, data, **kwargs):\r\n",
        "        z_mean, z_log_var, z = self.encoder(data)\r\n",
        "        reconstructed = self.decoder(z)\r\n",
        "        return reconstructed"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWJVg5WPFQlb"
      },
      "source": [
        "### Build VAE connecting Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfjVoHGu5eN_"
      },
      "source": [
        "### Define function to create model\n",
        "def create_model(intermediate_dim=140, dropout_rate=0.2, regularizer_rate=0.004, optimizer='adam', learn_rate=0.001,\n",
        "                 name='VAE'):\n",
        "    \"\"\"Creates VAE model, required for wrapping in estimator interface KerasRegressor, while accepting the hyperparameters we want to tune. We also pass some default values.\"\"\"\n",
        "\n",
        "    # create encoder \n",
        "    encoder = create_encoder(intermediate_dim=intermediate_dim, dropout_rate=dropout_rate,\n",
        "                             regularizer_rate=regularizer_rate)\n",
        "    # create decoder \n",
        "    decoder = create_decoder(intermediate_dim=intermediate_dim, dropout_rate=dropout_rate,\n",
        "                             regularizer_rate=regularizer_rate)\n",
        "    # create vae\n",
        "    model = VAE(encoder, decoder, name=name)\n",
        "    # compile model\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(lr=learn_rate, amsgrad=True)\n",
        "    else:\n",
        "        opt = SGD(lr=learn_rate)\n",
        "    model.compile(optimizer=opt)\n",
        "    model.build((None, 140, 1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-XPM-YdEiOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6545ad0-8d5f-44c2-c5d2-b6d5981e1555"
      },
      "source": [
        "### Instantiate VAE model\r\n",
        "vae = create_model(name='VAE')\r\n",
        "\r\n",
        "### Display VAE model and it`s parts\r\n",
        "# encoder \r\n",
        "vae.encoder.summary(line_length=100)\r\n",
        "plot_model(vae.encoder, show_shapes=True, to_file='vae_encoder.png')\r\n",
        "print(\"\\n\")\r\n",
        "# decoder\r\n",
        "vae.decoder.summary(line_length=100)\r\n",
        "plot_model(vae.decoder, show_shapes=True, to_file='vae_decoder.png')\r\n",
        "print(\"\\n\")\r\n",
        "# vae\r\n",
        "vae.summary(line_length=100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                     Output Shape          Param #     Connected to                     \n",
            "====================================================================================================\n",
            "Encoder_Input_layer (InputLayer) [(None, 140, 1)]      0                                            \n",
            "____________________________________________________________________________________________________\n",
            "Encode_1 (Bidirectional)         (None, 280)           159040      Encoder_Input_layer[0][0]        \n",
            "____________________________________________________________________________________________________\n",
            "Dropout_1 (Dropout)              (None, 280)           0           Encode_1[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "Encode_2 (Dense)                 (None, 5)             1405        Dropout_1[0][0]                  \n",
            "____________________________________________________________________________________________________\n",
            "z_mean (Dense)                   (None, 5)             30          Encode_2[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "z_log_var (Dense)                (None, 5)             30          Encode_2[0][0]                   \n",
            "____________________________________________________________________________________________________\n",
            "Sample_layer (Sampling)          (None, 5)             0           z_mean[0][0]                     \n",
            "                                                                   z_log_var[0][0]                  \n",
            "====================================================================================================\n",
            "Total params: 160,505\n",
            "Trainable params: 160,505\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Model: \"decoder\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                                 Output Shape                            Param #        \n",
            "====================================================================================================\n",
            "Decoder_Input_layer (InputLayer)             [(None, 5)]                             0              \n",
            "____________________________________________________________________________________________________\n",
            "Decode_1 (Dense)                             (None, 35840)                           215040         \n",
            "____________________________________________________________________________________________________\n",
            "Decode_2 (Reshape)                           (None, 140, 256)                        0              \n",
            "____________________________________________________________________________________________________\n",
            "Dropout_1 (Dropout)                          (None, 140, 256)                        0              \n",
            "____________________________________________________________________________________________________\n",
            "Decode_3 (Bidirectional)                     (None, 140, 280)                        444640         \n",
            "____________________________________________________________________________________________________\n",
            "Decoder_Output_Layer (TimeDistributed)       (None, 140, 1)                          281            \n",
            "====================================================================================================\n",
            "Total params: 659,961\n",
            "Trainable params: 659,961\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Model: \"VAE\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                                 Output Shape                            Param #        \n",
            "====================================================================================================\n",
            "encoder (Functional)                         [(None, 5), (None, 5), (None, 5)]       160505         \n",
            "____________________________________________________________________________________________________\n",
            "decoder (Functional)                         (None, 140, 1)                          659961         \n",
            "====================================================================================================\n",
            "Total params: 820,466\n",
            "Trainable params: 820,466\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7OlCIRCOUF-"
      },
      "source": [
        "# Train VAE\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf2gXbw290r5"
      },
      "source": [
        "### Train Properties\r\n",
        "epochs = 100 #50, 100\r\n",
        "batch_size = 16 #16, 32"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfnzjwIxlShE"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTkdJdc_N5SN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1765d9b3-c5fe-46bd-fe52-400c88163737"
      },
      "source": [
        "### Train\r\n",
        "train_history = vae.fit(x_train, x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 20s 44ms/step - loss: 48.6646 - reconstruction_loss: 47.3243 - kl_loss: 1.3404 - val_loss: 0.9088\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 37.3384 - reconstruction_loss: 35.5026 - kl_loss: 1.8357 - val_loss: 1.3079\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 35.4226 - reconstruction_loss: 33.3608 - kl_loss: 2.0618 - val_loss: 1.8159\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 34.0063 - reconstruction_loss: 31.8177 - kl_loss: 2.1885 - val_loss: 2.2392\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 32.7260 - reconstruction_loss: 30.3244 - kl_loss: 2.4016 - val_loss: 2.7451\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 31.1831 - reconstruction_loss: 28.5838 - kl_loss: 2.5993 - val_loss: 2.9848\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 30.1359 - reconstruction_loss: 27.3606 - kl_loss: 2.7753 - val_loss: 3.6670\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 29.1642 - reconstruction_loss: 26.2510 - kl_loss: 2.9132 - val_loss: 3.9958\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 28.2611 - reconstruction_loss: 25.2339 - kl_loss: 3.0272 - val_loss: 4.4521\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 27.6369 - reconstruction_loss: 24.5227 - kl_loss: 3.1141 - val_loss: 4.4417\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 27.4869 - reconstruction_loss: 24.3293 - kl_loss: 3.1576 - val_loss: 5.0808\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 26.2791 - reconstruction_loss: 23.0246 - kl_loss: 3.2545 - val_loss: 5.8290\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 26.1974 - reconstruction_loss: 22.8598 - kl_loss: 3.3376 - val_loss: 5.9607\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 25.1620 - reconstruction_loss: 21.8453 - kl_loss: 3.3168 - val_loss: 6.4468\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 25.5436 - reconstruction_loss: 22.1061 - kl_loss: 3.4375 - val_loss: 6.5071\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 24.2840 - reconstruction_loss: 20.8650 - kl_loss: 3.4189 - val_loss: 6.5753\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 23.7242 - reconstruction_loss: 20.3939 - kl_loss: 3.3303 - val_loss: 7.2552\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 23.5396 - reconstruction_loss: 20.1682 - kl_loss: 3.3715 - val_loss: 7.5206\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 23.4403 - reconstruction_loss: 19.9775 - kl_loss: 3.4628 - val_loss: 7.6112\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 23.0301 - reconstruction_loss: 19.6693 - kl_loss: 3.3608 - val_loss: 7.9899\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 22.6680 - reconstruction_loss: 19.3331 - kl_loss: 3.3349 - val_loss: 8.3272\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 22.4309 - reconstruction_loss: 19.0795 - kl_loss: 3.3515 - val_loss: 8.1420\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 22.3127 - reconstruction_loss: 18.9952 - kl_loss: 3.3174 - val_loss: 7.9751\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 21.9929 - reconstruction_loss: 18.6528 - kl_loss: 3.3401 - val_loss: 9.0002\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 22.0756 - reconstruction_loss: 18.7181 - kl_loss: 3.3574 - val_loss: 9.1342\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 21.3455 - reconstruction_loss: 18.0104 - kl_loss: 3.3352 - val_loss: 9.3606\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 21.2921 - reconstruction_loss: 17.9482 - kl_loss: 3.3439 - val_loss: 10.1441\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 21.1844 - reconstruction_loss: 17.7412 - kl_loss: 3.4432 - val_loss: 9.7435\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 21.0573 - reconstruction_loss: 17.5734 - kl_loss: 3.4839 - val_loss: 9.9659\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.9440 - reconstruction_loss: 17.5103 - kl_loss: 3.4337 - val_loss: 10.1796\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.8289 - reconstruction_loss: 17.3485 - kl_loss: 3.4804 - val_loss: 10.1643\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.9215 - reconstruction_loss: 17.4031 - kl_loss: 3.5183 - val_loss: 10.7190\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.5853 - reconstruction_loss: 17.0548 - kl_loss: 3.5305 - val_loss: 10.9883\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.3531 - reconstruction_loss: 16.8980 - kl_loss: 3.4550 - val_loss: 10.8732\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.1997 - reconstruction_loss: 16.6905 - kl_loss: 3.5092 - val_loss: 10.4730\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.2023 - reconstruction_loss: 16.7979 - kl_loss: 3.4044 - val_loss: 10.7752\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.1190 - reconstruction_loss: 16.6130 - kl_loss: 3.5060 - val_loss: 11.5122\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.0699 - reconstruction_loss: 16.5299 - kl_loss: 3.5400 - val_loss: 11.2691\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.9942 - reconstruction_loss: 16.4490 - kl_loss: 3.5452 - val_loss: 11.4758\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 20.1921 - reconstruction_loss: 16.7154 - kl_loss: 3.4767 - val_loss: 11.3300\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.8403 - reconstruction_loss: 16.4118 - kl_loss: 3.4285 - val_loss: 11.5368\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.7480 - reconstruction_loss: 16.2368 - kl_loss: 3.5112 - val_loss: 11.4376\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.7636 - reconstruction_loss: 16.2582 - kl_loss: 3.5055 - val_loss: 12.1901\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.6699 - reconstruction_loss: 16.0084 - kl_loss: 3.6615 - val_loss: 12.2950\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.4002 - reconstruction_loss: 15.8879 - kl_loss: 3.5123 - val_loss: 12.6906\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.5619 - reconstruction_loss: 15.9374 - kl_loss: 3.6246 - val_loss: 12.1352\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.3654 - reconstruction_loss: 15.8319 - kl_loss: 3.5335 - val_loss: 12.2869\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.2148 - reconstruction_loss: 15.7489 - kl_loss: 3.4659 - val_loss: 12.9493\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.2694 - reconstruction_loss: 15.7251 - kl_loss: 3.5444 - val_loss: 12.8486\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.2808 - reconstruction_loss: 15.6815 - kl_loss: 3.5993 - val_loss: 12.6290\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.2040 - reconstruction_loss: 15.5998 - kl_loss: 3.6043 - val_loss: 12.7671\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.1322 - reconstruction_loss: 15.5605 - kl_loss: 3.5717 - val_loss: 13.0938\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 19.1272 - reconstruction_loss: 15.5159 - kl_loss: 3.6114 - val_loss: 12.6217\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 10s 40ms/step - loss: 18.8504 - reconstruction_loss: 15.3584 - kl_loss: 3.4920 - val_loss: 12.5955\n",
            "Epoch 55/100\n",
            "207/250 [=======================>......] - ETA: 1s - loss: 18.7801 - reconstruction_loss: 15.3140 - kl_loss: 3.4661"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EOIKwwPnPgT"
      },
      "source": [
        "### Save history\r\n",
        "with open('/trainHistoryDict', 'wb') as file_pi:\r\n",
        "        pickle.dump(train_history.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHZuTNLVjUeh"
      },
      "source": [
        "### Check displayed values in the command line with actual output values of the trainings process\r\n",
        "print(\"----- loss: -----\\n{}\".format(train_history.history[\"loss\"]))\r\n",
        "print(\"----- reconstruction_loss: -----\\n{}\".format(train_history.history[\"reconstruction_loss\"]))\r\n",
        "print(\"----- kl_loss: -----\\n{}\".format(train_history.history[\"kl_loss\"]))\r\n",
        "print(\"----- val_loss: -----\\n{}\".format(train_history.history[\"val_loss\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljaYW8HuuHZk"
      },
      "source": [
        "## Recreate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYLbwD53cn84"
      },
      "source": [
        "# Encoder output is a list [z_mean, z_log_var, z] thus list[2] = z, see subsection encoder line 12\r\n",
        "\r\n",
        "### Extract myu i.e. z_mean\r\n",
        "z_mean = vae.encoder.predict(x_test)[0]\r\n",
        "print(\"----- z_mean: -----\")\r\n",
        "print(z_mean)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract sigma i.e. z_log_var\r\n",
        "z_log_var = vae.encoder.predict(x_test)[1]\r\n",
        "print(\"----- z_log_var: -----\")\r\n",
        "print(z_log_var)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract z_values and predict x_test\r\n",
        "z_values = vae.encoder.predict(x_test)[2]\r\n",
        "decoded_ecg5000 = vae.decoder.predict(z_values)\r\n",
        "# z_values contains list of each z_value per sample, i.e. we get 1000 SubLists with 5 elements in each.\r\n",
        "# Those 5 elements (z_values for Sample i) is our bottleneck which the decoder recieves.\r\n",
        "print(\"----- z_values: -----\")\r\n",
        "print(z_values)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Save extracted values\r\n",
        "np.savetxt('z_values.csv', z_values, delimiter=\",\")\r\n",
        "np.savetxt('decoded_ecg5000.csv', decoded_ecg5000.reshape(-1,140), delimiter=\",\")\r\n",
        "\r\n",
        "### Properties\r\n",
        "print(\"Shape and Type of z_mean: {}, {}\".format(z_mean.shape, type(z_mean)))\r\n",
        "print(\"Shape and Type of z_log_var: {}, {}\".format(z_log_var.shape, type(z_log_var)))\r\n",
        "print(\"Shape and Type of z_values: {}, {}\".format(z_values.shape, type(z_values)))\r\n",
        "print(\"Shape and Type of decoded_ecg5000: {}, {}\".format(decoded_ecg5000.shape, type(decoded_ecg5000)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNfzqeVE2N7J"
      },
      "source": [
        "## Display the training progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPE8qAylUlhq"
      },
      "source": [
        "#### Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ufa2OwJIVe"
      },
      "source": [
        "### Loss vs Reconstruction_loss vs KL Divergence\r\n",
        "plt.figure(figsize=(8,5))\r\n",
        "plt.plot(train_history.history['loss'])\r\n",
        "plt.plot(train_history.history['reconstruction_loss'])\r\n",
        "plt.plot(train_history.history['kl_loss'])\r\n",
        "plt.legend([\"Loss\", \"Reconstruction Loss\", \"KL Divergence\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Reconstruction Loss vs. KL Divergence\")\r\n",
        "\r\n",
        "plt.savefig('loss.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcIrsjRDYOiI"
      },
      "source": [
        "### Train loss vs val loss\r\n",
        "# returns the loss value & metrics values for the model in test mode\r\n",
        "plt.figure(figsize=(8,5))\r\n",
        "plt.plot(train_history.history['loss'])\r\n",
        "plt.plot(train_history.history['val_loss'])\r\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Validation Loss\")\r\n",
        "\r\n",
        "plt.savefig('valLoss.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGsZngmEUnKf"
      },
      "source": [
        "### Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldOtLNeJYJmb"
      },
      "source": [
        "### Scale Datan (PCA)\r\n",
        "# transform to dataframe\r\n",
        "z_test = pd.DataFrame(z_values)\r\n",
        "# standardize the data\r\n",
        "z_test = StandardScaler().fit_transform(z_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksY0r5zQsVjn"
      },
      "source": [
        "### Estimate how many components are needed to describe the data (PCA)\r\n",
        "pca_explained = PCA().fit(z_test)\r\n",
        "plt.plot(np.cumsum(pca_explaoned.explained_variance_ratio_))\r\n",
        "plt.xlabel('number of components')\r\n",
        "plt.ylabel('cumulative explained variance');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjILQXppYgIP"
      },
      "source": [
        "### PCA (5 dim -> 2 dim): display a 2D plot of the classes in the latent space.\r\n",
        "# make PCA instance\r\n",
        "pca = PCA(n_components=2)\r\n",
        "# fit transform features\r\n",
        "principalComponents = pca.fit_transform(z_test)\r\n",
        "# build pca dataframe\r\n",
        "principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\r\n",
        "targetDF = pd.DataFrame(data=testDF_Y.to_numpy(), columns=['target'])\r\n",
        "finalDF = pd.concat([principalDf, targetDF], axis=1)\r\n",
        "# scatterplot\r\n",
        "plt.figure(figsize=(8, 5))\r\n",
        "plt.xlabel('Principal Component 1')\r\n",
        "plt.ylabel('Principal Component 2')\r\n",
        "plt.title('Principal Component Analysis of Latent Space')\r\n",
        "plt.scatter(finalDF['principal component 1'], finalDF['principal component 2'], c=finalDF['target'],\r\n",
        "            cmap=plt.cm.get_cmap('Set1', 6), s=40, alpha=0.7)  # or cmap=hsv\r\n",
        "plt.colorbar(ticks=range(6), label='Classes of ECG500')\r\n",
        "plt.clim(-0.5, 5.5)\r\n",
        "\r\n",
        "plt.show()\r\n",
        "plt.savefig('PCA.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ8dyTK6kNNQ"
      },
      "source": [
        "# Plot Data Results\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfVmINR3lji"
      },
      "source": [
        "### Test if Input fits Dim of Output\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of decoded_ecg5000: {}\".format(decoded_ecg5000.shape))\n",
        "\n",
        "### Covert to 2D Array (\"-1\" = make a dimension (here rows) the size that will use the remaining unspecified elements)\n",
        "new_x_train= x_train.reshape(-1,140)\n",
        "new_decoded_ecg5000 = decoded_ecg5000.reshape(-1,140)\n",
        "\n",
        "print(\"Shape of Input after reshaping: {}\".format(new_x_train.shape))\n",
        "print(\"Shape of Output after reshaping: {}\".format(new_decoded_ecg5000.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG129YUrHGyU"
      },
      "source": [
        "# ### Plot figure for paper\r\n",
        "# i = 934 # sample which is going to be plotted\r\n",
        "# plt.figure(linewidth = 1, figsize=(25,6))\r\n",
        "# plt.xlabel('time steps')\r\n",
        "# plt.plot(new_x_train[i])\r\n",
        "# plt.show()\r\n",
        "# plt.savefig('diagramm_original.jpg')\r\n",
        "\r\n",
        "# plt.figure(linewidth = 1, figsize=(25,6))\r\n",
        "# plt.xlabel('time steps')\r\n",
        "# plt.plot(new_decoded_ecg5000[i], label='decoded ecg5000')\r\n",
        "# plt.show()\r\n",
        "# plt.savefig('diagramm_decoded.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGvzjGwcJi1B"
      },
      "source": [
        "### Plot only one sample\r\n",
        "i = 901  # sample which is going to be plotted\r\n",
        "plt.figure(linewidth=1, figsize=(20, 6))\r\n",
        "plt.title('Autoencoder Result')\r\n",
        "plt.xlabel('time steps')\r\n",
        "plt.plot(new_decoded_ecg5000[i], label='decoded ecg5000')\r\n",
        "plt.plot(new_x_train[i], label='original ecg5000')\r\n",
        "plt.legend(loc=\"upper left\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjUjaCtkSpW"
      },
      "source": [
        "### Plot Multiple Samples\n",
        "n_rows = 2\n",
        "n_cols = 3\n",
        "\n",
        "# size properties and layout design for tighter representation\n",
        "fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(13, 6))\n",
        "fig.tight_layout(w_pad=4, h_pad=5)\n",
        "\n",
        "# subplotting\n",
        "i = 50\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        axs[row, col].plot(new_decoded_ecg5000[i])\n",
        "        axs[row, col].plot(new_x_train[i])\n",
        "        axs[row, col].legend([\"Decoded ECG5000 Sample {}\".format(i), \"Original ECG5000 Sample {}\".format(i)])\n",
        "        axs[row, col].set(xlabel=\"Time Steps\", ylabel=\"Heartbeat Interpolated\", title=\"Sample {}\".format(i))\n",
        "        i = i + 75\n",
        "\n",
        "plt.savefig('dataComparison.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unWrNuYBc71O"
      },
      "source": [
        "# Optimization\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX8k4aeUCx3k"
      },
      "source": [
        "## Hyperparameter (Sckit_GridSearchCV)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywVsad-OqE7a"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "from sklearn.metrics import mean_squared_error, make_scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3NsIDD4dWHN"
      },
      "source": [
        "### Define scorer\r\n",
        "def score_mse(y_true, y_pred): # , **kwargs\r\n",
        "    \"\"\"Implementing mean squarred error as a score for RandomizedSearchCV.\"\"\"\r\n",
        "\r\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\r\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\r\n",
        "    # removing all size 1 dimensions in y_true\r\n",
        "    y_true = tf.squeeze(y_true)\r\n",
        "    return np.mean(tf.math.squared_difference(y_pred, y_true))\r\n",
        "\r\n",
        "### Define Function for Randomized Search\r\n",
        "def randomizedSearch_pipeline(x_train_data, x_test_data, model, space, n_iter=10, scoring_fit='neg_mean_squared_error',\r\n",
        "                              cv=5, do_probabilities=False):\r\n",
        "    \"\"\"Pipeline for RandomizedSearchCV: Select settings and run randomizedSearchCV, returning results.\"\"\"\r\n",
        "    # define randomizedSearch\r\n",
        "    rs = RandomizedSearchCV(\r\n",
        "        estimator=model,\r\n",
        "        param_distributions=space,\r\n",
        "        n_iter=n_iter,\r\n",
        "        scoring=scoring_fit,\r\n",
        "        n_jobs=1,\r\n",
        "        cv=cv,\r\n",
        "        verbose=2,\r\n",
        "        random_state=1,\r\n",
        "    )\r\n",
        "    # fit model\r\n",
        "    fitted_model = rs.fit(x_train_data, x_train_data, verbose=0)\r\n",
        "    # get results\r\n",
        "    rs_result = pd.DataFrame(rs.cv_results_)\r\n",
        "    # save compromised version of the results\r\n",
        "    min_rs_results = pd.concat([pd.DataFrame(rs.cv_results_[\"mean_test_score\"], columns=[\"score\"]),\r\n",
        "                                pd.DataFrame(rs.cv_results_[\"params\"])], axis=1)\r\n",
        "    min_rs_results = min_rs_results.sort_values(by=\"score\", ascending=False)\r\n",
        "    min_rs_results.to_latex(buf='randomizedSearchResults.tex', caption=(\r\n",
        "        \"Results of 20 candidates using a cross-validation of 5\", \"Randomized Search Results\"), label='table:1')\r\n",
        "\r\n",
        "    if do_probabilities:\r\n",
        "        pred = fitted_model.predict_proba(x_test_data)\r\n",
        "    else:\r\n",
        "        pred = fitted_model.predict(x_test_data)\r\n",
        "\r\n",
        "    return fitted_model, pred, rs_result, min_rs_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL_QfMgwCxfi"
      },
      "source": [
        "## Define evaluated params and it's value range\r\n",
        "space = {\r\n",
        "    'optimizer': ['adam', 'SGD'],\r\n",
        "    'batch_size': list(np.logspace(0, 6, 7, base=2, dtype=int)),\r\n",
        "    'dropout_rate': list(np.linspace(0, 1)),\r\n",
        "    'regularizer_rate': list(np.logspace(-6, -1, 6)),\r\n",
        "    'learn_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base=10, num=100)),\r\n",
        "}\r\n",
        "\r\n",
        "### Wrap keras custom VAE model with the KerasClassifier thus it implements estimator interface\r\n",
        "model = KerasRegressor(build_fn=create_model)\r\n",
        "\r\n",
        "### Run RandomizedSearch\r\n",
        "fitted_model, pred, rs_result, min_rs_results = randomizedSearch_pipeline(x_train, x_test, model, space, n_iter=4,\r\n",
        "                                                                          scoring_fit=make_scorer(score_mse,\r\n",
        "                                                                                                  greater_is_better=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjSWb0ldkIay"
      },
      "source": [
        "### Summarize results\r\n",
        "print(\"----- Results RandomizedSearchCV: -----\\n\" + \"Best: {} using {}\\n\".format(fitted_model.best_score_,\r\n",
        "                                                                                 fitted_model.best_params_))\r\n",
        "# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\r\n",
        "# pd.reset_option('all')\r\n",
        "print(\"Summary:\\n {}\".format(rs_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfBqVOpmdGJl"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZDGEMffcg0O"
      },
      "source": [
        "# ###Dropout_rate\r\n",
        "\r\n",
        "# # configure the experiment\r\n",
        "# def experiment_dropout():\r\n",
        "#   # configure the experiment\r\n",
        "#   n_dropout = [0.0, 0.2, 0.4, 0.6, 0.8]\r\n",
        "#   # run the experiment\r\n",
        "#   results = []\r\n",
        "#   for drop_value in n_dropout:\r\n",
        "#       # set dropout\r\n",
        "#       drop_out_rate = drop_value\r\n",
        "#       print(\"----- Dropout Rate: {} -----\".format(drop_out_rate))\r\n",
        "#       # evaluate\r\n",
        "#       # rather shorten code with defining a train function of code above and using it here\r\n",
        "#       vae = VAE(encoder, decoder, name=\"VAE\")\r\n",
        "#       vae.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "#       history = vae.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), verbose=0)\r\n",
        "#       # report performance\r\n",
        "#       # rather make a dataframe or something different which is simpler to plot\r\n",
        "#       evaluation = []\r\n",
        "#       evaluation.append(vae.evaluate(x_test, y_test))\r\n",
        "#       evaluation.append(drop_value)\r\n",
        "\r\n",
        "#       res = []\r\n",
        "#       res.append(history.history[\"val_loss\"])\r\n",
        "#       print(\"val_loss = {}\".format(res))\r\n",
        "#       results.append(evaluation)\r\n",
        "#   return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfM-TqopGGUu"
      },
      "source": [
        "# results = experiment_dropout()\r\n",
        "# # summarize results\r\n",
        "# print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0e7rRTv6STo"
      },
      "source": [
        "# Visualization of Hyperparameter Opt. Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRSEFKj93Q1F"
      },
      "source": [
        "Bar Plot\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE0Ev3GX3P7o"
      },
      "source": [
        "# Scores of our Hyperparameter Optimization\r\n",
        "scores = rs_result['mean_test_score'].tolist()\r\n",
        "\r\n",
        "# Positive Score, i.e. each score in scores * (-1)\r\n",
        "posScores = []\r\n",
        "for s in scores:\r\n",
        "    posScores.append(s * (-1))\r\n",
        "\r\n",
        "indices = np.arange(0, len(scores))\r\n",
        "\r\n",
        "plt.bar(indices, posScores, tick_label=indices)\r\n",
        "plt.title(\"Score of each parameter combination\")\r\n",
        "plt.xlabel(\"Unit\")\r\n",
        "plt.ylabel(\"Score\")\r\n",
        "# save fig\r\n",
        "plt.savefig(fname=\"scores.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwY-GoamH-6e"
      },
      "source": [
        "Scatter Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr1nvfOdQ_ym"
      },
      "source": [
        "### features\r\n",
        "features = ['batch_size', 'dropout_rate', 'regularizer_rate', 'learn_rate']\r\n",
        "\r\n",
        "fig, axs = plt.subplots(nrows=len(features), ncols=len(features), figsize=(12, 12))\r\n",
        "fig.tight_layout(w_pad=2, h_pad=2)\r\n",
        "\r\n",
        "col = 0\r\n",
        "for feature_1 in features:\r\n",
        "    row = 0\r\n",
        "    for feature_2 in features:\r\n",
        "        bestComb = min_rs_results.iloc[0:5]\r\n",
        "        rest = min_rs_results.iloc[5:]\r\n",
        "        axs[row, col].scatter(bestComb[feature_1], bestComb[feature_2], color='green', alpha=0.7)\r\n",
        "        axs[row, col].scatter(rest[feature_1], rest[feature_2], color='red', alpha=0.7)\r\n",
        "        axs[row, col].set_xlabel(feature_1, fontsize=9)\r\n",
        "        axs[row, col].set_ylabel(feature_2, fontsize=9)\r\n",
        "        row = row + 1\r\n",
        "    col = col + 1\r\n",
        "\r\n",
        "# save fig\r\n",
        "plt.savefig('scoresScatter.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}