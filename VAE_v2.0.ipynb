{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_v1_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nina-prog/DataAnalysis_VAE/blob/main/VAE_v2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srk9OHyCZDzL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Reshape\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcUlg_PrMiyC"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9i4M00iwYs"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcrTqGPsZceo"
      },
      "source": [
        "### Load ecg5000 data using read_csv\n",
        "ecg5000 = pd.read_csv('ECG5000_ALL.txt', sep='\\s+', header=None)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbiFe4pZlQv"
      },
      "source": [
        "### Optional test and info about data set\n",
        "print(\"Type of ecg5000: \\t \\t {}\".format(type(ecg5000)))\n",
        "print(\"Dimensions of ecg5000: \\t \\t {}\".format(ecg5000.shape))\n",
        "print(\"Number of elements of ecg5000: \\t {}\".format((ecg5000.size)))\n",
        "print(\"Display first 10 rows of ecg5000: \\n {}\".format(ecg5000.head(10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvRjcSqjFED"
      },
      "source": [
        "### Get YLabel df\n",
        "dfYLabel = ecg5000.iloc[:,0]\n",
        "\n",
        "### Normalize dataframe with min-max-normalization to range between [-0.8, 0.8] using sklearn MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-0.8,0.8))\n",
        "scaled_ecg5000 = pd.DataFrame(min_max_scaler.fit_transform(ecg5000))\n",
        "print(scaled_ecg5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Qxc-o8i2cV"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FuU0-HYavQx"
      },
      "source": [
        "### Split Data into 80/20 Training, Test\n",
        "trainDF, testDF = train_test_split(scaled_ecg5000, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Get all labels from trainDF and then drop it\n",
        "trainDF_Y = trainDF.iloc[:,0]\n",
        "trainDF.drop(trainDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# Get all labels from testDF and then drop it\n",
        "testDF_Y = testDF.iloc[:,0]\n",
        "testDF.drop(testDF.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Optional test and info about new data sets\n",
        "print(\"Shape of Train DataFrame: \\t {}\".format(trainDF.shape))\n",
        "print(\"Shape of Test DataFrame: \\t {}\".format(testDF.shape))\n",
        "print(\"Shape of Train Y DataFrame: \\t {}\".format(trainDF_Y.shape))\n",
        "print(\"Shape of Test Y DataFrame: \\t {}\".format(testDF_Y.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqDAiFlPipdc"
      },
      "source": [
        "## Reshape Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkCjqkHiHBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6536fc3-c0ed-41c2-d22b-0d86fbbd51ee"
      },
      "source": [
        "### Convert to array\n",
        "x_train = trainDF.to_numpy()\n",
        "x_test = testDF.to_numpy()\n",
        "\n",
        "y_train = trainDF_Y.to_numpy()\n",
        "y_test = testDF_Y.to_numpy()\n",
        "\n",
        "### Reshape datasets X/Y train/test into [samples, timesteps, features]\n",
        "s_xtrain = len(trainDF.index) # samples\n",
        "n_xtrain = len(trainDF.columns) # time steps\n",
        "\n",
        "s_xtest = len(testDF.index) # samples\n",
        "n_xtest = len(testDF.columns) # time steps\n",
        "\n",
        "s_ytrain = len(trainDF_Y.index) # samples\n",
        "\n",
        "s_ytest = len(testDF_Y.index) # samples\n",
        "\n",
        "x_train = x_train.reshape(s_xtrain, n_xtrain, 1)\n",
        "x_test = x_test.reshape(s_xtest, n_xtest, 1)\n",
        "\n",
        "y_train = y_train.reshape(s_ytrain, 1, 1)\n",
        "y_test = y_test.reshape(s_ytest, 1, 1)\n",
        "\n",
        "### Properties\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
        "\n",
        "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
        "print(\"Shape of y_test: {}\".format(y_test.shape))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_train: (4000, 140, 1)\n",
            "Shape of x_test: (1000, 140, 1)\n",
            "Shape of y_train: (4000, 1, 1)\n",
            "Shape of y_test: (1000, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeXLn2bUYa7g"
      },
      "source": [
        "# Create Sample Layer\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuO7o5GGYg3y"
      },
      "source": [
        "class Sampling(layers.Layer):\r\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z\"\"\"\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var = inputs\r\n",
        "        batch = tf.shape(z_mean)[0]\r\n",
        "        dim = tf.shape(z_mean)[1]\r\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F9x-KyKM4NJ"
      },
      "source": [
        "# Build Variational Autoencoder (VAE)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmlHrviGtyZ"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_pSJx0oFlvB"
      },
      "source": [
        "### For better understanding visit: https://towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e\r\n",
        "### For better understanding of layers and Recreating auto encoders visit: https://machinelearningmastery.com/lstm-autoencoders/\r\n",
        "### or for code: https://gist.github.com/GerardBCN/40349b39bc45d4550141aff6966d1619#file-stock_price_autoencoding-ipynb\r\n",
        "### For Reshaping Issues: https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\r\n",
        "\r\n",
        "### Model Properties\r\n",
        "encoding_dim = 140\r\n",
        "classes_dim = 5 #5, because 5 class in data ecg5000 - evtl 2,1\r\n",
        "latent_dim = 5\r\n",
        "\r\n",
        "epochs = 30\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "drop_out_rate = 0.2"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hcpCe0zGwQl"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4194a4eCBSh5"
      },
      "source": [
        "### Define Encoder Layers\r\n",
        "encoder_inputs = keras.Input(shape=(140, 1), name='Encoder_Input_layer')\r\n",
        "\r\n",
        "encoded = Dropout(drop_out_rate, name='Dropout_1')(encoder_inputs)\r\n",
        "encoded = Bidirectional(LSTM(encoding_dim, activation='tanh', name=''), name='Encode_1')(encoded)\r\n",
        "encoded = Dropout(drop_out_rate, name='Dropout_2')(encoded)\r\n",
        "encoded = Dense(classes_dim, activation='tanh', name='Encode_2')(encoded) \r\n",
        "z_mean = Dense(latent_dim, activation=\"softplus\", name=\"z_mean\")(encoded) \r\n",
        "z_log_var = Dense(latent_dim, activation=\"softplus\", name=\"z_log_var\")(encoded)\r\n",
        "\r\n",
        "z = Sampling(name='Sample_layer')([z_mean, z_log_var])\r\n",
        "\r\n",
        "### Build Encoder\r\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\r\n",
        "encoder.summary()\r\n",
        "# configure encoder for training\r\n",
        "encoder.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "\r\n",
        "plot_model(encoder, show_shapes=True, to_file='reconstruct_lstm_encoder.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_PrqOCOG244"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxnUeH7FBJVH"
      },
      "source": [
        "### Define Decoder Layers\r\n",
        "latent_inputs = keras.Input(shape=(latent_dim,), name='Decoder_Input_layer')\r\n",
        "\r\n",
        "decoded = Dense(140, activation='tanh', name='Decode_1')(latent_inputs)\r\n",
        "decoded = Reshape((140,1), name='Decode_2')(decoded)\r\n",
        "decoded = Dropout(drop_out_rate, name='Dropout_1')(decoded)\r\n",
        "decoded = Bidirectional(LSTM(encoding_dim, return_sequences=True, activation='tanh', name=''), name='Decode_3')(decoded)\r\n",
        "decoded = Dropout(drop_out_rate, name='Dropout_2')(decoded)\r\n",
        "\r\n",
        "decoder_outputs = TimeDistributed(Dense(1, activation='tanh', name=''),name='Decoder_Output_Layer')(decoded)\r\n",
        "\r\n",
        "### Build Decoder\r\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\r\n",
        "decoder.summary()\r\n",
        "# configure decoder for training\r\n",
        "decoder.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "\r\n",
        "plot_model(decoder, show_shapes=True, to_file='reconstruct_lstm_decoder.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAKUqMR8oSm6"
      },
      "source": [
        "## Connecting the Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR-wmYsdQXII"
      },
      "source": [
        "### For better understanding visit https://becominghuman.ai/using-variational-autoencoder-vae-to-generate-new-images-14328877e88d\r\n",
        "\r\n",
        "### Instantiate VAE model\r\n",
        "# The output of vae model is the output of decoder in which its input is taken from the output of encoder !\r\n",
        "decoder_outputs = decoder(encoder(encoder_inputs)[2])\r\n",
        "vae = keras.Model(encoder_inputs, decoder_outputs, name='vae_v2_0')\r\n",
        "\r\n",
        "vae.summary()\r\n",
        "plot_model(vae, show_shapes=True, to_file='reconstruct_lstm_variational_autoencoder.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7OlCIRCOUF-"
      },
      "source": [
        "# Train VAE\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1m0bKlZlIuL"
      },
      "source": [
        "## Define Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9hxGlDYBBKV"
      },
      "source": [
        "class VAE(keras.Model):\r\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, encoder, decoder, **kwargs):\r\n",
        "        super(VAE, self).__init__(**kwargs)\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        if isinstance(data, tuple):\r\n",
        "            data = data[0]\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            z_mean, z_log_var, z = encoder(data)\r\n",
        "            reconstruction = decoder(z)\r\n",
        "            # reconstruction_loss = distance between Input and Output\r\n",
        "            reconstruction_loss = tf.reduce_mean(\r\n",
        "                #Alternative: keras.losses.binary_crossentropy(data, reconstruction)\r\n",
        "                keras.losses.mean_squared_error(data, reconstruction)\r\n",
        "            )\r\n",
        "            # kl_loss = distance between distributions and thus ensures the regular laten space\r\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\r\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\r\n",
        "            kl_loss *= -0.5\r\n",
        "            total_loss = reconstruction_loss + kl_loss\r\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\r\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\r\n",
        "        return {\r\n",
        "            \"loss\": total_loss,\r\n",
        "            \"reconstruction_loss\": reconstruction_loss,\r\n",
        "            \"kl_loss\": kl_loss,\r\n",
        "        }\r\n",
        "\r\n",
        "    def call(self, data):\r\n",
        "        z_mean, z_log_var, z = self.encoder(data)\r\n",
        "        reconstructed = self.decoder(z)\r\n",
        "        return reconstructed"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfnzjwIxlShE"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTkdJdc_N5SN"
      },
      "source": [
        "### To improve training see here: https://becominghuman.ai/using-variational-autoencoder-vae-to-generate-new-images-14328877e88d\r\n",
        " \r\n",
        "### Train\r\n",
        "vae = VAE(encoder, decoder, name=\"VAE\")\r\n",
        "vae.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "history = vae.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QQwKfBZwqJG"
      },
      "source": [
        "Test drop_out_rate\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjDX0HAkwpzV"
      },
      "source": [
        "###Dropout_rate\r\n",
        "\r\n",
        "# configure the experiment\r\n",
        "def experiment_dropout():\r\n",
        "  # configure the experiment\r\n",
        "  n_dropout = [0.0, 0.2, 0.4, 0.6, 0.8]\r\n",
        "  # run the experiment\r\n",
        "  results = []\r\n",
        "  for drop_value in n_dropout:\r\n",
        "      # set dropout\r\n",
        "      drop_out_rate = drop_value\r\n",
        "      print(\"----- Dropout Rate: {} -----\".format(drop_out_rate))\r\n",
        "      # evaluate\r\n",
        "      vae = VAE(encoder, decoder, name=\"VAE\")\r\n",
        "      vae.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "      history = vae.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), verbose=0)\r\n",
        "      # report performance\r\n",
        "      # rathr make a dataframe or something different which is simpler to plot\r\n",
        "      evaluation = []\r\n",
        "      evaluation.append(vae.evaluate(x_test, y_test))\r\n",
        "      evaluation.append(drop_value)\r\n",
        "      print(\"val_loss = {}\".format(evaluation))\r\n",
        "      results.append(evaluation)\r\n",
        "  return results"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfM-TqopGGUu"
      },
      "source": [
        "results = experiment_dropout()\r\n",
        "# summarize results\r\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sUJNF_pJ-rv",
        "outputId": "1eb23ac4-ee1e-469f-c845-55301fcc9c15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(results)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.31067535281181335, 0.0, 0.30874761939048767, 0.2, 0.3046853840351105, 0.4, 0.3114849030971527, 0.6], [0.31067535281181335, 0.0, 0.30874761939048767, 0.2, 0.3046853840351105, 0.4, 0.3114849030971527, 0.6], [0.31067535281181335, 0.0, 0.30874761939048767, 0.2, 0.3046853840351105, 0.4, 0.3114849030971527, 0.6], [0.31067535281181335, 0.0, 0.30874761939048767, 0.2, 0.3046853840351105, 0.4, 0.3114849030971527, 0.6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljaYW8HuuHZk"
      },
      "source": [
        "## Recreate Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYLbwD53cn84"
      },
      "source": [
        "# Encoder output is a list [z_mean, z_log_var, z] thus list[2] = z, see subsection encoder line 12\r\n",
        "\r\n",
        "### Extract myu i.e. z_mean\r\n",
        "z_mean = encoder.predict(x_test)[0]\r\n",
        "print(\"----- z_mean: -----\")\r\n",
        "print(z_mean)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract sigma i.e. z_log_var\r\n",
        "z_log_var = encoder.predict(x_test)[1]\r\n",
        "print(\"----- z_log_var: -----\")\r\n",
        "print(z_log_var)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Extract z_values and predict x_test\r\n",
        "z_values = encoder.predict(x_test)[2]\r\n",
        "# decoded_ecg5000 = vae.predict(x_test)\r\n",
        "decoded_ecg5000 = decoder.predict(z_values)\r\n",
        "\r\n",
        "# z_values contains list of each z_value per sample, i.e. we get 1000 SubLists with 5 elements in each.\r\n",
        "# Those 5 elements (z_values for Sample i) is our bottleneck which the decoder\r\n",
        "# recieves.\r\n",
        "print(\"----- z_values: -----\")\r\n",
        "print(z_values)\r\n",
        "print(\"\\n\")\r\n",
        "\r\n",
        "### Properties\r\n",
        "print(\"Shape of z_mean: {}\".format(z_mean.shape))\r\n",
        "print(\"Shape of z_log_var: {}\".format(z_log_var.shape))\r\n",
        "print(\"Shape of decoded_ecg5000: {}\".format(decoded_ecg5000.shape))\r\n",
        "print(\"Shape of z_values: {}\".format(z_values.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNfzqeVE2N7J"
      },
      "source": [
        "## Display the training progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ufa2OwJIVe"
      },
      "source": [
        "### Loss vs Reconstruction_loss\r\n",
        "plt.figure(figsize=(16,8))\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['reconstruction_loss'])\r\n",
        "plt.legend([\"Loss\", \"Reconstruction Loss\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Reconstruction Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcIrsjRDYOiI"
      },
      "source": [
        "### Train loss vs val loss\r\n",
        "#Returns the loss value & metrics values for the model in test mode\r\n",
        "\r\n",
        "plt.figure(figsize=(16,8))\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.title(\"Loss vs. Validation Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSBgUNCkM05C"
      },
      "source": [
        "##################################################################################### TODO: Display latent space\r\n",
        "### Encode data into latent space and show the distribution via normal plot\r\n",
        "\r\n",
        "plt.figure(figsize=(30,12), linewidth=1)\r\n",
        "plt.plot(z_values[:,0], 'o')\r\n",
        "plt.plot(z_values[:,1], '+')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "## ... via Scatterplot\r\n",
        "# *i unpacks i into a tuple (i[0], i[1]), which is interpreted as (x,y) by plt.scatter\r\n",
        "# for i in bottleneck:\r\n",
        "#   plt.scatter(*i)\r\n",
        "\r\n",
        "plt.figure(figsize=(14,12))\r\n",
        "plt.scatter(z_values[:,0], z_values[:,1])\r\n",
        "#plt.colorbar()\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ8dyTK6kNNQ"
      },
      "source": [
        "# Plot Results\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfVmINR3lji"
      },
      "source": [
        "### Test if Input fits Dim of Output\n",
        "print(\"Shape of Input x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of Output x_hat_train: {}\".format(decoded_ecg5000.shape))\n",
        "\n",
        "### Covert to 2D Array -- (\"-1\" = make a dimension (here rows) the size that will use the remaining unspecified elements)\n",
        "new_x_train= x_train.reshape(-1,140)\n",
        "new_decoded_ecg5000 = decoded_ecg5000.reshape(-1,140)\n",
        "\n",
        "print(\"Shape of Input after reshaping: {}\".format(new_x_train.shape))\n",
        "print(\"Shape of Output after reshaping: {}\".format(new_decoded_ecg5000.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG129YUrHGyU"
      },
      "source": [
        "### Plot only one sample\r\n",
        "i = 934 # indize/sample which is going to be plotted\r\n",
        "plt.figure(linewidth = 1, figsize=(25,6))\r\n",
        "plt.title('Autoencoder Result')\r\n",
        "plt.xlabel('time steps')\r\n",
        "plt.plot(new_x_train[i], label='original ecg5000')\r\n",
        "plt.plot(new_decoded_ecg5000[i], label='decoded ecg5000')\r\n",
        "plt.legend(loc=\"upper left\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjUjaCtkSpW"
      },
      "source": [
        "### Plot Multiple Samples\n",
        "n_rows = 2                   \n",
        "n_cols = 2\n",
        "\n",
        "# Size Properties and layout design for tighter representation\n",
        "fig, axs = plt.subplots(nrows = n_rows, ncols = n_cols, figsize=(23,20))\n",
        "fig.tight_layout(w_pad=4, h_pad = 5)\n",
        "\n",
        "# Subplotting\n",
        "i = 100\n",
        "for row in range(n_rows):\n",
        "  for col in range(n_cols):\n",
        "    axs[row, col].plot(new_x_train[i])\n",
        "    axs[row, col].plot(new_decoded_ecg5000[i])\n",
        "    axs[row, col].legend([\"Original ECG5000 Sample {}\".format(i), \"Decoded ECG5000 Sample {}\".format(i)])\n",
        "    axs[row, col].set(xlabel = \"Time Steps\", ylabel = \"Heartbeat Interpolated\", title = \"Sample {}\".format(i))\n",
        "    i = i + 200\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}